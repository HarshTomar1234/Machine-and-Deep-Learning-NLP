{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leOBR8Ypky7j"
      },
      "outputs": [],
      "source": [
        "# fibonacci Series\n",
        "\n",
        "import time\n",
        "def fib(n):\n",
        "    if n == 1 or n == 0:\n",
        "        return 1\n",
        "    else:\n",
        "      return fib(n-1) + fib(n-2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "fib(36)\n",
        "print(time.time() -  start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvdJ0ihFlZVi",
        "outputId": "318773a8-13f3-4a2a-cd51-9cfdf79105ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.104997634887695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In deep learning, **memoization** refers to an optimization technique used to improve computational efficiency by storing the results of expensive function calls and reusing them when the same inputs occur again. While memoization is more commonly associated with dynamic programming in algorithm design, its principles can be applied in certain areas of deep learning as well.\n",
        "\n",
        "### How Memoization Applies to Deep Learning:\n",
        "\n",
        "1. **Caching Intermediate Results**:\n",
        "   - In deep learning frameworks (e.g., TensorFlow, PyTorch), memoization can be used to cache intermediate computations. For example, during backpropagation, some frameworks cache the forward pass values (activations) to avoid recomputing them for gradient calculations.\n",
        "   \n",
        "2. **Reusing Computations in Recurrent Networks**:\n",
        "   - Recurrent Neural Networks (RNNs) and their variants (like LSTMs, GRUs) process sequences by iteratively applying the same operations. Memoization can help reuse previously computed results for overlapping subsequences.\n",
        "\n",
        "3. **Dynamic Programming and Sequence Models**:\n",
        "   - In tasks like sequence alignment, parsing, or dynamic neural networks, memoization helps in storing subproblem solutions, avoiding redundant computations.\n",
        "\n",
        "4. **Hyperparameter Optimization**:\n",
        "   - During hyperparameter tuning, if the same set of hyperparameters is evaluated more than once, memoization avoids retraining the model by reusing previously computed results.\n",
        "\n",
        "5. **Accelerating Inference with Precomputed Results**:\n",
        "   - In applications like search or recommendation systems, precomputing and caching results for commonly seen inputs can reduce latency.\n",
        "\n",
        "### Benefits:\n",
        "- **Speed**: Reduces redundant computations, improving speed.\n",
        "- **Efficiency**: Saves resources, especially in complex models with overlapping computations.\n",
        "  \n",
        "### Challenges:\n",
        "- **Memory Overhead**: Storing intermediate results can increase memory usage.\n",
        "- **Consistency and Cache Management**: Ensuring the cache remains valid and efficient requires careful management.\n",
        "\n"
      ],
      "metadata": {
        "id": "WE-t6YbMlvWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memoization in Deep Learning: Mathematical Framework\n",
        "\n",
        "## Overview\n",
        "Memoization is an optimization technique that stores previously computed results to avoid redundant calculations in deep learning algorithms. It's particularly useful in backpropagation and recursive neural computations.\n",
        "\n",
        "## 1. Forward Pass Computations\n",
        "\n",
        "For a neural network layer $l$, the forward pass computations are:\n",
        "\n",
        "$z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$\n",
        "\n",
        "$a^{(l)} = \\sigma(z^{(l)})$\n",
        "\n",
        "Where:\n",
        "- $W^{(l)}$ = weights of layer $l$\n",
        "- $b^{(l)}$ = biases of layer $l$\n",
        "- $a^{(l)}$ = activation output of layer $l$\n",
        "- $\\sigma$ = activation function\n",
        "- $z^{(l)}$ = pre-activation values\n",
        "\n",
        "## 2. Backpropagation with Memoization\n",
        "\n",
        "During backpropagation, we compute:\n",
        "\n",
        "$\\delta^{(l)} = (W^{(l+1)})^\\top \\delta^{(l+1)} \\odot \\sigma'(z^{(l)})$\n",
        "\n",
        "$\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} = \\delta^{(l)} (a^{(l-1)})^\\top$\n",
        "\n",
        "Instead of recomputing $z^{(l)}$ and $a^{(l)}$, memoization stores these values from the forward pass.\n",
        "\n",
        "## 3. Dynamic Programming in Sequence Models\n",
        "\n",
        "For sequence models like HMMs, memoization helps in computing path probabilities:\n",
        "\n",
        "$V(t, s) = \\max_{s'} [V(t-1, s') \\cdot P(x_t | s) \\cdot P(s | s')]$\n",
        "\n",
        "Where:\n",
        "- $V(t, s)$ = probability of most probable path ending in state $s$ at time $t$\n",
        "- $P(x_t | s)$ = emission probability\n",
        "- $P(s | s')$ = transition probability\n",
        "\n",
        "## 4. RNN Computations\n",
        "\n",
        "For Recurrent Neural Networks:\n",
        "\n",
        "$h_t = f(W_h h_{t-1} + W_x x_t + b)$\n",
        "\n",
        "$o_t = W_o h_t + c$\n",
        "\n",
        "Memoization stores $h_t$ values to avoid recomputation when:\n",
        "$h_{t+k} = h_t$ (for identical inputs in deterministic RNNs)\n",
        "\n",
        "## 5. Benefits and Complexity\n",
        "\n",
        "Time Complexity Reduction:\n",
        "$T_{\\text{with memoization}} < T_{\\text{without memoization}}$\n",
        "\n",
        "Space-Time Tradeoff:\n",
        "$\\mathcal{O}(T \\times S)$ with memoization vs $\\mathcal{O}(T^S)$ without memoization\n",
        "\n",
        "Where:\n",
        "- $T$ = sequence length\n",
        "- $S$ = state space size\n",
        "\n",
        "## Implementation Considerations\n",
        "\n",
        "1. Cache stored values during forward pass\n",
        "2. Reuse cached values during backward pass\n",
        "3. Clear cache when no longer needed\n",
        "4. Balance memory usage vs computational savings"
      ],
      "metadata": {
        "id": "o2hsI6S5lvaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def fib(n, d):\n",
        "    if n in d:\n",
        "        return d[n]\n",
        "    else:\n",
        "      d[n] = fib(n-1, d) + fib(n-2, d)\n",
        "      return d[n]\n"
      ],
      "metadata": {
        "id": "ibP9jLrHlvkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "d = {0:0, 1:1}\n",
        "fib(100, d)\n",
        "print(time.time() -  start)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzS5WyIFpRHd",
        "outputId": "6c917f53-8c3a-48cc-dd5c-8651ee65f337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0003273487091064453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1. **Caching in Backpropagation**\n",
        "\n",
        "We’ll use PyTorch to show how caching forward pass results improves backpropagation efficiency.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.linear1 = nn.Linear(10, 20)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(20, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        z1 = self.linear1(x)\n",
        "        a1 = self.relu(z1)\n",
        "        z2 = self.linear2(a1)\n",
        "        return z1, a1, z2\n",
        "\n",
        "# Example of forward and backward pass with caching\n",
        "model = SimpleNN()\n",
        "x = torch.randn(5, 10)  # Batch size of 5, input dimension of 10\n",
        "z1, a1, z2 = model(x)  # Forward pass\n",
        "\n",
        "# Caching forward pass results\n",
        "cache = {\"z1\": z1, \"a1\": a1, \"z2\": z2}\n",
        "\n",
        "# Compute loss\n",
        "target = torch.randn(5, 1)\n",
        "criterion = nn.MSELoss()\n",
        "loss = criterion(z2, target)\n",
        "loss.backward()  # Backpropagation\n",
        "```\n",
        "\n",
        "### 2. **Dynamic Programming in Sequence Models**\n",
        "\n",
        "Below is a Viterbi algorithm for finding the most probable sequence of states in an HMM, using memoization.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Transition probabilities\n",
        "transition_prob = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
        "# Emission probabilities\n",
        "emission_prob = np.array([[0.9, 0.1], [0.2, 0.8]])\n",
        "# Initial probabilities\n",
        "start_prob = np.array([0.6, 0.4])\n",
        "\n",
        "def viterbi(obs, states, start_prob, trans_prob, emit_prob):\n",
        "    T = len(obs)\n",
        "    N = len(states)\n",
        "    \n",
        "    # Memoization table\n",
        "    memo = np.zeros((T, N))\n",
        "    path = np.zeros((T, N), dtype=int)\n",
        "    \n",
        "    # Initialization\n",
        "    memo[0, :] = start_prob * emit_prob[:, obs[0]]\n",
        "    \n",
        "    # Dynamic Programming\n",
        "    for t in range(1, T):\n",
        "        for s in range(N):\n",
        "            prob = memo[t-1, :] * trans_prob[:, s] * emit_prob[s, obs[t]]\n",
        "            memo[t, s] = np.max(prob)\n",
        "            path[t, s] = np.argmax(prob)\n",
        "    \n",
        "    # Traceback\n",
        "    best_path = [np.argmax(memo[-1, :])]\n",
        "    for t in range(T-1, 0, -1):\n",
        "        best_path.insert(0, path[t, best_path[0]])\n",
        "    \n",
        "    return best_path, np.max(memo[-1, :])\n",
        "\n",
        "# Observations and states\n",
        "observations = [0, 1, 0]\n",
        "states = [0, 1]\n",
        "\n",
        "best_seq, best_prob = viterbi(observations, states, start_prob, transition_prob, emission_prob)\n",
        "print(\"Best sequence:\", best_seq)\n",
        "print(\"Best probability:\", best_prob)\n",
        "```\n",
        "\n",
        "### 3. **RNN Hidden State Reuse**\n",
        "\n",
        "Below is an example where hidden states in an RNN are memoized to avoid redundant computations for repeated inputs.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "    \n",
        "    def forward(self, x, h0=None):\n",
        "        return self.rnn(x, h0)\n",
        "\n",
        "rnn = SimpleRNN(10, 20)\n",
        "x = torch.randn(5, 3, 10)  # Batch size 5, sequence length 3, input size 10\n",
        "\n",
        "# Memoization for hidden states\n",
        "hidden_cache = {}\n",
        "\n",
        "def memoized_forward(rnn, x, cache):\n",
        "    h0 = cache.get('h0', None)\n",
        "    out, hn = rnn(x, h0)\n",
        "    cache['h0'] = hn  # Save the hidden state\n",
        "    return out, hn\n",
        "\n",
        "out, hn = memoized_forward(rnn, x, hidden_cache)\n",
        "print(\"Output:\", out)\n",
        "print(\"Cached Hidden State:\", hidden_cache['h0'])\n",
        "```\n",
        "\n",
        "### 4. **Hyperparameter Optimization with Memoization**\n",
        "\n",
        "Here’s a grid search for hyperparameter tuning with memoization to avoid redundant evaluations.\n",
        "\n",
        "```python\n",
        "import random\n",
        "\n",
        "# Function to simulate model evaluation\n",
        "def evaluate_model(hyperparams):\n",
        "    lr, batch_size = hyperparams\n",
        "    # Simulated expensive function (e.g., training a model)\n",
        "    random.seed(hash((lr, batch_size)) % 100000)\n",
        "    return random.uniform(0.7, 0.9)  # Simulated accuracy\n",
        "\n",
        "# Memoization table\n",
        "evaluation_cache = {}\n",
        "\n",
        "def memoized_evaluate(hyperparams, cache):\n",
        "    if hyperparams in cache:\n",
        "        print(\"Using cached result.\")\n",
        "        return cache[hyperparams]\n",
        "    else:\n",
        "        result = evaluate_model(hyperparams)\n",
        "        cache[hyperparams] = result\n",
        "        return result\n",
        "\n",
        "# Hyperparameter grid\n",
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "batch_sizes = [32, 64, 128]\n",
        "\n",
        "# Grid search with memoization\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "        params = (lr, batch_size)\n",
        "        accuracy = memoized_evaluate(params, evaluation_cache)\n",
        "        print(f\"Params: {params}, Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
       
