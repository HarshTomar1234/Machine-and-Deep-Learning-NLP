{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech Of DR APJ Abdul Kalam\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\dlib-19.24.6-py3.12-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\face_recognition-1.3.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\playsound-1.3.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo',\n",
       " 'como',\n",
       " 'más',\n",
       " 'pero',\n",
       " 'sus',\n",
       " 'le',\n",
       " 'ya',\n",
       " 'o',\n",
       " 'este',\n",
       " 'sí',\n",
       " 'porque',\n",
       " 'esta',\n",
       " 'entre',\n",
       " 'cuando',\n",
       " 'muy',\n",
       " 'sin',\n",
       " 'sobre',\n",
       " 'también',\n",
       " 'me',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'donde',\n",
       " 'quien',\n",
       " 'desde',\n",
       " 'todo',\n",
       " 'nos',\n",
       " 'durante',\n",
       " 'todos',\n",
       " 'uno',\n",
       " 'les',\n",
       " 'ni',\n",
       " 'contra',\n",
       " 'otros',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'ante',\n",
       " 'ellos',\n",
       " 'e',\n",
       " 'esto',\n",
       " 'mí',\n",
       " 'antes',\n",
       " 'algunos',\n",
       " 'qué',\n",
       " 'unos',\n",
       " 'yo',\n",
       " 'otro',\n",
       " 'otras',\n",
       " 'otra',\n",
       " 'él',\n",
       " 'tanto',\n",
       " 'esa',\n",
       " 'estos',\n",
       " 'mucho',\n",
       " 'quienes',\n",
       " 'nada',\n",
       " 'muchos',\n",
       " 'cual',\n",
       " 'poco',\n",
       " 'ella',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'algunas',\n",
       " 'algo',\n",
       " 'nosotros',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'tú',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'ellas',\n",
       " 'nosotras',\n",
       " 'vosotros',\n",
       " 'vosotras',\n",
       " 'os',\n",
       " 'mío',\n",
       " 'mía',\n",
       " 'míos',\n",
       " 'mías',\n",
       " 'tuyo',\n",
       " 'tuya',\n",
       " 'tuyos',\n",
       " 'tuyas',\n",
       " 'suyo',\n",
       " 'suya',\n",
       " 'suyos',\n",
       " 'suyas',\n",
       " 'nuestro',\n",
       " 'nuestra',\n",
       " 'nuestros',\n",
       " 'nuestras',\n",
       " 'vuestro',\n",
       " 'vuestra',\n",
       " 'vuestros',\n",
       " 'vuestras',\n",
       " 'esos',\n",
       " 'esas',\n",
       " 'estoy',\n",
       " 'estás',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'esté',\n",
       " 'estés',\n",
       " 'estemos',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estaré',\n",
       " 'estarás',\n",
       " 'estará',\n",
       " 'estaremos',\n",
       " 'estaréis',\n",
       " 'estarán',\n",
       " 'estaría',\n",
       " 'estarías',\n",
       " 'estaríamos',\n",
       " 'estaríais',\n",
       " 'estarían',\n",
       " 'estaba',\n",
       " 'estabas',\n",
       " 'estábamos',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estuve',\n",
       " 'estuviste',\n",
       " 'estuvo',\n",
       " 'estuvimos',\n",
       " 'estuvisteis',\n",
       " 'estuvieron',\n",
       " 'estuviera',\n",
       " 'estuvieras',\n",
       " 'estuviéramos',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuviese',\n",
       " 'estuvieses',\n",
       " 'estuviésemos',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estando',\n",
       " 'estado',\n",
       " 'estada',\n",
       " 'estados',\n",
       " 'estadas',\n",
       " 'estad',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ha',\n",
       " 'hemos',\n",
       " 'habéis',\n",
       " 'han',\n",
       " 'haya',\n",
       " 'hayas',\n",
       " 'hayamos',\n",
       " 'hayáis',\n",
       " 'hayan',\n",
       " 'habré',\n",
       " 'habrás',\n",
       " 'habrá',\n",
       " 'habremos',\n",
       " 'habréis',\n",
       " 'habrán',\n",
       " 'habría',\n",
       " 'habrías',\n",
       " 'habríamos',\n",
       " 'habríais',\n",
       " 'habrían',\n",
       " 'había',\n",
       " 'habías',\n",
       " 'habíamos',\n",
       " 'habíais',\n",
       " 'habían',\n",
       " 'hube',\n",
       " 'hubiste',\n",
       " 'hubo',\n",
       " 'hubimos',\n",
       " 'hubisteis',\n",
       " 'hubieron',\n",
       " 'hubiera',\n",
       " 'hubieras',\n",
       " 'hubiéramos',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubiese',\n",
       " 'hubieses',\n",
       " 'hubiésemos',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'habiendo',\n",
       " 'habido',\n",
       " 'habida',\n",
       " 'habidos',\n",
       " 'habidas',\n",
       " 'soy',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'somos',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'seas',\n",
       " 'seamos',\n",
       " 'seáis',\n",
       " 'sean',\n",
       " 'seré',\n",
       " 'serás',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'seréis',\n",
       " 'serán',\n",
       " 'sería',\n",
       " 'serías',\n",
       " 'seríamos',\n",
       " 'seríais',\n",
       " 'serían',\n",
       " 'era',\n",
       " 'eras',\n",
       " 'éramos',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'fui',\n",
       " 'fuiste',\n",
       " 'fue',\n",
       " 'fuimos',\n",
       " 'fuisteis',\n",
       " 'fueron',\n",
       " 'fuera',\n",
       " 'fueras',\n",
       " 'fuéramos',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fuese',\n",
       " 'fueses',\n",
       " 'fuésemos',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'sintiendo',\n",
       " 'sentido',\n",
       " 'sentida',\n",
       " 'sentidos',\n",
       " 'sentidas',\n",
       " 'siente',\n",
       " 'sentid',\n",
       " 'tengo',\n",
       " 'tienes',\n",
       " 'tiene',\n",
       " 'tenemos',\n",
       " 'tenéis',\n",
       " 'tienen',\n",
       " 'tenga',\n",
       " 'tengas',\n",
       " 'tengamos',\n",
       " 'tengáis',\n",
       " 'tengan',\n",
       " 'tendré',\n",
       " 'tendrás',\n",
       " 'tendrá',\n",
       " 'tendremos',\n",
       " 'tendréis',\n",
       " 'tendrán',\n",
       " 'tendría',\n",
       " 'tendrías',\n",
       " 'tendríamos',\n",
       " 'tendríais',\n",
       " 'tendrían',\n",
       " 'tenía',\n",
       " 'tenías',\n",
       " 'teníamos',\n",
       " 'teníais',\n",
       " 'tenían',\n",
       " 'tuve',\n",
       " 'tuviste',\n",
       " 'tuvo',\n",
       " 'tuvimos',\n",
       " 'tuvisteis',\n",
       " 'tuvieron',\n",
       " 'tuviera',\n",
       " 'tuvieras',\n",
       " 'tuviéramos',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuviese',\n",
       " 'tuvieses',\n",
       " 'tuviésemos',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'teniendo',\n",
       " 'tenido',\n",
       " 'tenida',\n",
       " 'tenidos',\n",
       " 'tenidas',\n",
       " 'tened']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords And Filter And then Apply Stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'i believ india got first vision 1857 , start war independ .',\n",
       " 'it freedom must protect nurtur build .',\n",
       " 'if free , one respect us .',\n",
       " 'my second vision india ’ develop .',\n",
       " 'for fifti year develop nation .',\n",
       " 'it time see develop nation .',\n",
       " 'we among top 5 nation world term gdp .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverti level fall .',\n",
       " 'our achiev global recognis today .',\n",
       " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
       " 'isn ’ incorrect ?',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus i believ unless india stand world , one respect us .',\n",
       " 'onli strength respect strength .',\n",
       " 'we must strong militari power also econom power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i see four mileston career']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowballstemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords And Filter And then Apply Snowball Stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india .',\n",
       " '3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'alexand onward , greek , turk , mogul , portugu , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'conquer anyon .',\n",
       " 'grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'believ india got first vision 1857 , start war independ .',\n",
       " 'freedom must protect nurtur build .',\n",
       " 'free , one respect us .',\n",
       " 'second vision india ’ develop .',\n",
       " 'fifti year develop nation .',\n",
       " 'time see develop nation .',\n",
       " 'among top 5 nation world term gdp .',\n",
       " '10 percent growth rate area .',\n",
       " 'poverti level fall .',\n",
       " 'achiev global recogni today .',\n",
       " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
       " '’ incorrect ?',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus believ unless india stand world , one respect us .',\n",
       " 'on strength respect strength .',\n",
       " 'must strong militari power also econom power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'lucki work three close consid great opportun life .',\n",
       " 'see four mileston career']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords And Filter And then Apply lemmatization\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    #sentences[i]=sentences[i].lower()\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india .',\n",
       " '3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'alexand onward , greek , turk , mogul , portugu , british , french , dutch , come loot us , take .',\n",
       " 'yet do nation .',\n",
       " 'conquer anyon .',\n",
       " 'grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'believ india get first vision 1857 , start war independ .',\n",
       " 'freedom must protect nurtur build .',\n",
       " 'free , one respect us .',\n",
       " 'second vision india ’ develop .',\n",
       " 'fifti year develop nation .',\n",
       " 'time see develop nation .',\n",
       " 'among top 5 nation world term gdp .',\n",
       " '10 percent growth rate area .',\n",
       " 'poverti level fall .',\n",
       " 'achiev global recogni today .',\n",
       " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
       " '’ incorrect ?',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus believ unless india stand world , one respect us .',\n",
       " 'strength respect strength .',\n",
       " 'must strong militari power also econom power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'lucki work three close consid great opportun life .',\n",
       " 'see four mileston career']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let get some extra - insights!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What are Stopwords in NLP?**\n",
    "\n",
    "In **Natural Language Processing (NLP)**, **stopwords** refer to common words in any language that typically carry minimal informational value and are often removed in text processing. Examples of stopwords in English include words like *\"is,\" \"the,\" \"and,\"* and *\"in.\"* Stopwords are generally high-frequency words that act as syntactic glue in a sentence rather than providing significant meaning. By filtering out stopwords, NLP applications can focus on the more meaningful parts of the text.\n",
    "\n",
    "### **Why Remove Stopwords?**\n",
    "Stopwords removal is a crucial step in NLP because:\n",
    "1. **Reduces Noise**: Stopwords don't usually contribute much to the meaning or sentiment of the text, so removing them can reduce noise and improve model performance.\n",
    "2. **Enhances Computational Efficiency**: Fewer words mean reduced dimensionality and faster processing, as algorithms have fewer terms to process.\n",
    "3. **Improves Relevance**: It allows NLP models to focus on content words, which typically carry more significant meaning and thus improve tasks like text classification, search relevance, and summarization.\n",
    "\n",
    "### **Examples of Stopwords**\n",
    "- **English**: \"a,\" \"an,\" \"the,\" \"and,\" \"is,\" \"in,\" \"at,\" \"on,\" etc.\n",
    "- **French**: \"le,\" \"la,\" \"et,\" \"dans,\" \"est,\" etc.\n",
    "- **Hindi**: \"है,\" \"को,\" \"में,\" \"से,\" \"के,\" etc.\n",
    "\n",
    "Each language has its own set of stopwords, as common words differ across languages.\n",
    "\n",
    "### **Application of Stopwords in NLP Tasks**\n",
    "\n",
    "1. **Text Classification**: By removing stopwords, only the most meaningful words remain, helping improve the performance of classifiers (e.g., in spam detection or sentiment analysis).\n",
    "2. **Information Retrieval and Search Engines**: Stopwords removal enhances search algorithms by focusing on keywords that actually affect search relevance.\n",
    "3. **Summarization**: Removing stopwords helps extract key phrases and generate more concise summaries.\n",
    "4. **Topic Modeling**: In tasks like **Latent Dirichlet Allocation (LDA)**, removing stopwords improves topic coherence by focusing on more relevant terms.\n",
    "5. **Machine Translation**: Some translation algorithms remove or prioritize stopwords differently to improve translation accuracy and computational efficiency.\n",
    "\n",
    "### **Stopword Removal Techniques**\n",
    "\n",
    "1. **Library-Based Removal**:\n",
    "   Libraries like **NLTK**, **spaCy**, and **scikit-learn** provide predefined lists of stopwords for various languages.\n",
    "   \n",
    "   - **Example in Python with NLTK**:\n",
    "     ```python\n",
    "     from nltk.corpus import stopwords\n",
    "     from nltk.tokenize import word_tokenize\n",
    "\n",
    "     # Define the stopword list\n",
    "     stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "     # Sample text\n",
    "     text = \"This is an example sentence demonstrating the removal of stopwords.\"\n",
    "\n",
    "     # Tokenize text\n",
    "     words = word_tokenize(text)\n",
    "\n",
    "     # Filter out stopwords\n",
    "     filtered_sentence = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "     print(\"Original:\", words)\n",
    "     print(\"Filtered:\", filtered_sentence)\n",
    "     ```\n",
    "   - **Output**:\n",
    "     ```\n",
    "     Original: ['This', 'is', 'an', 'example', 'sentence', 'demonstrating', 'the', 'removal', 'of', 'stopwords', '.']\n",
    "     Filtered: ['example', 'sentence', 'demonstrating', 'removal', 'stopwords']\n",
    "     ```\n",
    "\n",
    "2. **Custom Stopword Lists**:\n",
    "   Some projects require custom stopword lists based on specific terminology or domain requirements. For instance, in a financial dataset, words like \"bank\" or \"market\" might be so common that they become irrelevant to certain tasks, even though they aren’t considered traditional stopwords.\n",
    "\n",
    "   - **Example Code**:\n",
    "     ```python\n",
    "     # Define a custom list of stopwords\n",
    "     custom_stopwords = set([\"example\", \"sentence\", \"removal\"])\n",
    "\n",
    "     # Sample text\n",
    "     text = \"This is an example sentence demonstrating the removal of stopwords.\"\n",
    "\n",
    "     # Tokenize and filter based on custom stopwords\n",
    "     words = word_tokenize(text)\n",
    "     filtered_sentence = [word for word in words if word.lower() not in custom_stopwords]\n",
    "\n",
    "     print(\"Custom Filtered:\", filtered_sentence)\n",
    "     ```\n",
    "   - **Output**:\n",
    "     ```\n",
    "     Custom Filtered: ['This', 'is', 'an', 'demonstrating', 'the', 'of', 'stopwords', '.']\n",
    "     ```\n",
    "\n",
    "3. **Frequency-Based Stopwords**:\n",
    "   In some cases, words that occur extremely frequently within a specific corpus can be automatically removed, as they add little value for the analysis. For example, if a word appears in 90% of the documents in a corpus, it may be designated as a stopword.\n",
    "\n",
    "4. **Contextual Stopwords**:\n",
    "   In more advanced NLP systems, stopword lists may be adjusted based on context. For instance, in sentiment analysis, words like \"very\" and \"not\" might normally be considered stopwords but carry important sentiment modifiers, so they are retained.\n",
    "\n",
    "### **Challenges with Stopwords**\n",
    "\n",
    "1. **Loss of Contextual Meaning**: Some stopwords, especially negatives (e.g., *\"not\"*, *\"without\"*), can modify the meaning of a sentence. Removing them may cause a loss of nuance, especially in tasks like sentiment analysis.\n",
    "2. **Domain-Specific Stopwords**: Certain words may be very common in specific fields (e.g., \"data\" in data science articles) but crucial for interpretation.\n",
    "3. **Handling Multilingual Stopwords**: For multilingual datasets, you need stopword lists for each language, making the processing more complex.\n",
    "\n",
    "### **Stopwords in NLP Libraries**\n",
    "\n",
    "Most NLP libraries provide tools for working with stopwords:\n",
    "\n",
    "1. **NLTK**:\n",
    "   - Provides stopword lists in multiple languages.\n",
    "   - Commonly used in beginner-to-intermediate projects due to its extensive tools and educational purpose.\n",
    "\n",
    "2. **spaCy**:\n",
    "   - Offers an extensive list of stopwords and provides language models with built-in stopword support for multiple languages.\n",
    "   - Often chosen for production-level applications due to speed and efficiency.\n",
    "\n",
    "   ```python\n",
    "   import spacy\n",
    "\n",
    "   # Load English tokenizer, tagger, parser, and NER\n",
    "   nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "   # Define sample text\n",
    "   text = \"This is an example sentence demonstrating the removal of stopwords.\"\n",
    "\n",
    "   # Process text with spaCy\n",
    "   doc = nlp(text)\n",
    "\n",
    "   # Filter out stopwords\n",
    "   filtered_sentence = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "   print(\"Filtered with spaCy:\", filtered_sentence)\n",
    "   ```\n",
    "   \n",
    "3. **Gensim**:\n",
    "   - Commonly used in topic modeling (e.g., LDA) and provides tools to handle stopwords within topic modeling tasks.\n",
    "\n",
    "4. **scikit-learn**:\n",
    "   - Has a built-in list of English stopwords in its `CountVectorizer` and `TfidfVectorizer` classes, commonly used for text feature extraction.\n",
    "\n",
    "5. **Hugging Face Transformers**:\n",
    "   - While transformer models generally encode stopwords in the context of surrounding words, they can still benefit from stopwords filtering in pre-processing.\n",
    "\n",
    "### **When Not to Remove Stopwords**\n",
    "\n",
    "While stopwords removal is usually beneficial, there are cases where it may be detrimental:\n",
    "- **Text Summarization**: Removing stopwords may change the core message of a text, so they are often retained.\n",
    "- **Machine Translation**: For translating the original meaning accurately, stopwords should be retained.\n",
    "- **Sentiment Analysis**: Words like “not” or “very” play a crucial role in determining sentiment, so stopwords filtering must be customized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are a critical concept in **Natural Language Processing (NLP)**, as they help streamline text data by removing common, often less informative words. Removing these \"stopwords\" allows NLP algorithms to focus on words that carry more significant meaning in a sentence.\n",
    "\n",
    "Here’s an in-depth look at handling stopwords across various NLP libraries: **NLTK**, **spaCy**, **scikit-learn**, **Gensim**, and **Hugging Face Transformers**.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Using NLTK for Stopword Removal**\n",
    "\n",
    "NLTK (Natural Language Toolkit) provides a simple yet effective way to handle stopwords in several languages.\n",
    "\n",
    "### Installation:\n",
    "```bash\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "### Example Code:\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the stopwords set\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Sample text\n",
    "text = \"This is an example sentence to demonstrate the removal of stopwords using NLTK.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original:\", words)\n",
    "print(\"Filtered:\", filtered_words)\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "Original: ['This', 'is', 'an', 'example', 'sentence', 'to', 'demonstrate', 'the', 'removal', 'of', 'stopwords', 'using', 'NLTK', '.']\n",
    "Filtered: ['example', 'sentence', 'demonstrate', 'removal', 'stopwords', 'NLTK', '.']\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "1. **Tokenization**: The `word_tokenize` function splits the text into words.\n",
    "2. **Filtering**: Words in `stop_words` are removed, keeping only the meaningful ones.\n",
    "\n",
    "### Customizing Stopwords\n",
    "You can add or remove words from the default stopwords list if the task requires specific adjustments.\n",
    "```python\n",
    "# Adding a custom stopword\n",
    "stop_words.add(\"example\")\n",
    "\n",
    "filtered_words_custom = [word for word in words if word.lower() not in stop_words]\n",
    "print(\"Filtered with Custom Stopwords:\", filtered_words_custom)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Using spaCy for Stopword Removal**\n",
    "\n",
    "spaCy is a high-performance NLP library that supports stopwords natively and allows for faster, more efficient processing.\n",
    "\n",
    "### Installation:\n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "### Example Code:\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define sample text\n",
    "text = \"This is an example sentence to demonstrate the removal of stopwords using spaCy.\"\n",
    "\n",
    "# Process text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Filter out stopwords\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "print(\"Filtered with spaCy:\", filtered_words)\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "Filtered with spaCy: ['example', 'sentence', 'demonstrate', 'removal', 'stopwords', 'spaCy', '.']\n",
    "```\n",
    "\n",
    "### Customizing Stopwords in spaCy\n",
    "spaCy allows adding or removing stopwords from its built-in list.\n",
    "```python\n",
    "# Add a new stopword\n",
    "nlp.Defaults.stop_words.add(\"example\")\n",
    "\n",
    "# Remove a stopword\n",
    "nlp.Defaults.stop_words.remove(\"not\")\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "1. **`token.is_stop`**: Checks if each token is a stopword. This attribute allows for quick filtering.\n",
    "2. **Custom Stopwords**: spaCy allows you to customize the list by adding or removing words based on your dataset’s needs.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Using scikit-learn for Stopword Removal**\n",
    "\n",
    "scikit-learn offers stopwords as part of its `CountVectorizer` and `TfidfVectorizer` classes, which are often used for feature extraction in machine learning pipelines.\n",
    "\n",
    "### Installation:\n",
    "```bash\n",
    "pip install scikit-learn\n",
    "```\n",
    "\n",
    "### Example Code:\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text\n",
    "text = [\"This is an example sentence to demonstrate the removal of stopwords using scikit-learn.\"]\n",
    "\n",
    "# Define CountVectorizer with stopwords\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(text)\n",
    "\n",
    "print(\"Filtered words using scikit-learn CountVectorizer:\", vectorizer.get_feature_names_out())\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "Filtered words using scikit-learn CountVectorizer: ['demonstrate', 'example', 'removal', 'sentence', 'stopwords', 'using']\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "1. **CountVectorizer with Stopwords**: `stop_words=\"english\"` automatically removes common English stopwords.\n",
    "2. **Custom Stopwords**: You can also define custom stopwords by passing a list to `stop_words`.\n",
    "\n",
    "### Example with Custom Stopwords:\n",
    "```python\n",
    "custom_stopwords = [\"example\", \"demonstrate\"]\n",
    "\n",
    "vectorizer_custom = CountVectorizer(stop_words=custom_stopwords)\n",
    "X_custom = vectorizer_custom.fit_transform(text)\n",
    "\n",
    "print(\"Filtered with Custom Stopwords:\", vectorizer_custom.get_feature_names_out())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Using Gensim for Stopword Removal**\n",
    "\n",
    "Gensim is a library commonly used for topic modeling (e.g., LDA) and also provides support for stopword removal.\n",
    "\n",
    "### Installation:\n",
    "```bash\n",
    "pip install gensim\n",
    "```\n",
    "\n",
    "### Example Code:\n",
    "```python\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# Define sample text\n",
    "text = \"This is an example sentence to demonstrate the removal of stopwords using Gensim.\"\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_text = remove_stopwords(text)\n",
    "\n",
    "print(\"Filtered with Gensim:\", filtered_text)\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "Filtered with Gensim: example sentence demonstrate removal stopwords Gensim.\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "1. **`remove_stopwords`**: Gensim’s `remove_stopwords` function directly removes stopwords from a given string.\n",
    "2. **No Tokenization Needed**: Unlike NLTK or spaCy, Gensim removes stopwords without tokenizing, making it suitable for quick filtering tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Using Hugging Face Transformers for Stopword Removal**\n",
    "\n",
    "The **Hugging Face Transformers** library does not natively handle stopwords but integrates well with other libraries (e.g., NLTK) to handle stopwords in NLP pipelines involving transformer models.\n",
    "\n",
    "### Installation:\n",
    "```bash\n",
    "pip install transformers nltk\n",
    "```\n",
    "\n",
    "### Example Code:\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define sample text\n",
    "text = \"This is an example sentence to demonstrate the removal of stopwords with Transformers.\"\n",
    "\n",
    "# Tokenize using Hugging Face\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Filter out stopwords\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "print(\"Filtered with Transformers:\", filtered_tokens)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "1. **Tokenization**: Hugging Face tokenizer splits text into subwords (sub-tokenization) to handle complex vocabulary.\n",
    "2. **Stopwords Removal**: Since Hugging Face doesn’t natively handle stopwords, this example combines it with NLTK to remove stopwords after tokenization.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison of Different Libraries for Stopword Removal**\n",
    "\n",
    "| **Library**      | **Stopwords Feature**                                              | **Custom Stopwords**                  | **Typical Use Case**                                      |\n",
    "|------------------|--------------------------------------------------------------------|---------------------------------------|-----------------------------------------------------------|\n",
    "| **NLTK**         | Extensive list in multiple languages                              | Yes                                   | Academic projects, beginners, traditional NLP tasks       |\n",
    "| **spaCy**        | Built-in stopwords with language models                           | Yes (add or remove)                   | Industrial applications, high-performance NLP pipelines   |\n",
    "| **scikit-learn** | Built-in stopwords with CountVectorizer, TfidfVectorizer          | Yes                                   | Machine learning pipelines, text classification           |\n",
    "| **Gensim**       | Quick stopwords removal for text preprocessing                    | Limited customization                 | Topic modeling (e.g., LDA)                                |\n",
    "| **Transformers** | No built-in stopwords; relies on other libraries (e.g., NLTK)     | Limited to external libraries         | Transformer-based NLP models with custom stopword handling|\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "Each library offers unique strengths:\n",
    "- **NLTK** is best for educational and exploratory purposes with extensive stopword lists.\n",
    "- **spaCy** is ideal for production-grade projects needing efficient, customizable stopword handling.\n",
    "- **scikit-learn** is valuable in ML workflows, especially in vectorization.\n",
    "- **Gensim** is highly optimized for topic modeling and quick preprocessing.\n",
    "- **Transformers with Hugging Face** is preferred for deep learning tasks and can work with NLTK for stopword customization.\n",
    "\n",
    "Choosing the right library depends on your task's complexity and computational needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
