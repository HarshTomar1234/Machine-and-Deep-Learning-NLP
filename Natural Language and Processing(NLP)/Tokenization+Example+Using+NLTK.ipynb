{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of NLTK and spaCy\n",
    "\n",
    "#### **NLTK (Natural Language Toolkit):**\n",
    "**NLTK** is one of the oldest and most comprehensive libraries for working with human language data (text) in Python. It provides a wide range of tools and resources for text processing, such as tokenization, stemming, lemmatization, part-of-speech (POS) tagging, parsing, and more. Additionally, NLTK includes a large collection of text corpora and lexical resources, which makes it a powerful tool for linguistic research and natural language processing (NLP) tasks.\n",
    "\n",
    "##### Key Features of NLTK:\n",
    "1. **Corpora and Datasets:** Comes with over 50 corpora (e.g., WordNet, Brown Corpus) and lexical resources, making it ideal for research and prototyping.\n",
    "2. **Versatile Text Processing Functions:** Provides extensive tools for text cleaning, stemming, lemmatization, tokenization, tagging, chunking, parsing, and semantic reasoning.\n",
    "3. **Text Classification:** It supports both supervised and unsupervised learning for tasks like document classification, named entity recognition (NER), and more.\n",
    "4. **Parsing & Grammar Tools:** Provides support for both shallow and deep parsing techniques using Context-Free Grammars (CFG), dependency parsing, etc.\n",
    "5. **Rich Ecosystem:** Due to its long history and broad scope, NLTK is widely used in academia for teaching NLP concepts, with numerous tutorials and references available.\n",
    "\n",
    "#### **spaCy:**\n",
    "**spaCy** is a more recent library, designed to be efficient and production-ready for real-world NLP applications. Unlike NLTK, which is more focused on research and learning, spaCy is optimized for speed and ease of use in industry settings. It is designed to work well with large-scale data and is integrated with machine learning frameworks like TensorFlow and PyTorch.\n",
    "\n",
    "##### Key Features of spaCy:\n",
    "1. **Performance and Speed:** spaCy is built with efficiency in mind, using Cython to handle computationally heavy operations, making it faster than most libraries.\n",
    "2. **Pre-trained Models:** Provides state-of-the-art pre-trained models for various languages that support tasks like tokenization, POS tagging, dependency parsing, and named entity recognition (NER).\n",
    "3. **Deep Learning Integration:** Easily integrates with deep learning frameworks (TensorFlow, PyTorch) and includes functionality for training and fine-tuning models.\n",
    "4. **Out-of-the-box NLP Pipelines:** Includes a well-defined and easy-to-extend NLP pipeline that handles all steps of text processing (tokenization, NER, etc.) in a streamlined way.\n",
    "5. **Modern Linguistic Features:** spaCy has advanced support for dependency parsing, word vectors, part-of-speech tagging, and much more. It also provides support for modern techniques like word embeddings (via word2vec, GloVe, etc.).\n",
    "\n",
    "### Differences Between NLTK and spaCy:\n",
    "\n",
    "| **Feature**                | **NLTK**                               | **spaCy**                            |\n",
    "|----------------------------|----------------------------------------|--------------------------------------|\n",
    "| **Release Date**            | Older (first released in 2001)         | Newer (first released in 2015)       |\n",
    "| **Target Audience**         | Researchers, educators, and prototyping| Developers, industry practitioners   |\n",
    "| **Focus**                   | Focus on learning, research, and prototyping | Focus on performance, production, and real-world applications |\n",
    "| **Ease of Use**             | Offers a more academic, modular approach; may require combining various tools manually | Out-of-the-box solutions and streamlined NLP pipelines |\n",
    "| **Performance**             | Slower, not optimized for large-scale data processing | Optimized for high performance with large datasets |\n",
    "| **Pre-trained Models**      | Requires users to train many models; fewer pre-trained models out-of-the-box | Provides robust, pre-trained models (NER, POS, etc.) for various languages |\n",
    "| **Deep Learning**           | Minimal support for deep learning frameworks | Directly integrates with TensorFlow, PyTorch |\n",
    "| **Tokenization**            | Simple rule-based tokenization         | More robust tokenization with better handling of edge cases (via pre-trained models) |\n",
    "| **Language Support**        | Supports many languages via corpora    | Supports multiple languages with specific, pre-trained models |\n",
    "| **Training Custom Models**  | Complex and not as well documented     | Easier to customize and train models, with extensive documentation |\n",
    "| **Parsing and Syntax Trees**| Rich support for different types of parsing (e.g., CFG, dependency parsing) | Dependency parsing is streamlined and fast |\n",
    "| **Stemming & Lemmatization**| Provides traditional stemming algorithms (e.g., Porter Stemmer) | Lemmatization is more accurate due to pre-trained models |\n",
    "| **Community and Ecosystem** | Older, larger academic community with many tutorials and research papers | Growing ecosystem, especially in industry NLP applications |\n",
    "\n",
    "### When to Use NLTK vs. spaCy:\n",
    "- **Use NLTK if:**\n",
    "  - You are learning NLP or working on a research project.\n",
    "  - You need access to a wide variety of text corpora and linguistic data.\n",
    "  - You want to experiment with different NLP techniques, such as different tokenizers or parsers.\n",
    "  \n",
    "- **Use spaCy if:**\n",
    "  - You are working on a large-scale NLP application and need fast, efficient processing.\n",
    "  - You want easy access to pre-trained models for tasks like NER, POS tagging, etc.\n",
    "  - You want to integrate NLP tasks with deep learning models and frameworks.\n",
    "  - You are focused on production-level applications rather than exploratory data analysis.\n",
    "\n",
    "Both libraries are powerful, but spaCy is preferred for production systems due to its speed and ease of use, while NLTK is ideal for educational purposes and linguistic exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP (Natural Language Processing), several fundamental concepts are often used when working with text data. Let’s specify terms like **corpus**, **documents**, **vocabulary**, and **words**:\n",
    "\n",
    "### 1. **Corpus**:\n",
    "A **corpus** (plural: corpora) refers to a large and structured collection of text data, usually in digital form, that is used for linguistic analysis, text mining, or training NLP models. It serves as the base material on which NLP models are trained or analyzed.\n",
    "\n",
    "- **Example of Corpora:**\n",
    "  - **Brown Corpus**: A large corpus of American English text.\n",
    "  - **Wikipedia Dump**: A snapshot of all Wikipedia articles, often used for training NLP models.\n",
    "  - **Movie Reviews Corpus**: A collection of movie reviews, often used for sentiment analysis tasks.\n",
    "\n",
    "In summary, a corpus is a collection of text samples used in research or model training. It could be as small as a set of articles or as large as a database of billions of sentences.\n",
    "\n",
    "### 2. **Documents**:\n",
    "A **document** in NLP refers to an individual piece of text within a corpus. A document could be an article, a paragraph, a tweet, a webpage, a research paper, or any single unit of text.\n",
    "\n",
    "- **Example:**\n",
    "  - In a **news corpus**, each article is considered a document.\n",
    "  - In a **Twitter dataset**, each tweet is treated as a document.\n",
    "\n",
    "Documents are essentially subsets of a corpus.\n",
    "\n",
    "### 3. **Vocabulary**:\n",
    "The **vocabulary** refers to the set of unique words (or tokens) that appear in a corpus. It includes all distinct terms or symbols in the text data. The vocabulary is often generated by tokenizing the corpus (splitting text into words or subwords) and keeping track of unique terms.\n",
    "\n",
    "- **Example:**\n",
    "  If a corpus consists of the following sentences:\n",
    "  - \"The cat sat on the mat.\"\n",
    "  - \"The dog chased the cat.\"\n",
    "  \n",
    "  The vocabulary would include: `{\"The\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"chased\"}` (unique words, ignoring duplicates).\n",
    "  \n",
    "  In many cases, NLP preprocessing involves further reducing the vocabulary by removing common stopwords (like \"the\", \"on\") and applying techniques like lemmatization or stemming.\n",
    "\n",
    "### 4. **Words** (Tokens):\n",
    "In NLP, **words** are the smallest individual units of meaning in the text. These are also called **tokens**. Tokenization is the process of splitting text into individual words or tokens. Each token typically represents a word in the document, though in some cases, a token could be a punctuation mark or even a subword (for example, in languages with complex morphology).\n",
    "\n",
    "- **Example:**\n",
    "  The sentence \"The cat sat on the mat.\" can be tokenized into the following words/tokens:\n",
    "  - [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]\n",
    "\n",
    "A **word** in NLP can be:\n",
    "- A real word from a sentence (\"cat\", \"sat\").\n",
    "- A punctuation mark (\"!\").\n",
    "- A subword if advanced tokenization techniques are used (such as BPE or WordPiece tokenization used in models like BERT).\n",
    "\n",
    "### Example Using All Terms Together:\n",
    "Let’s consider a practical example:\n",
    "1. You have a **corpus** of 1000 news articles.\n",
    "2. Each news article is a **document** in the corpus.\n",
    "3. After processing the text, the **vocabulary** consists of 10,000 unique words.\n",
    "4. The total number of **words** (or tokens) in the corpus is 500,000 (this includes repeated instances of words).\n",
    "\n",
    "### Summary of Definitions:\n",
    "- **Corpus**: A collection of text data.\n",
    "- **Documents**: Individual pieces of text within the corpus.\n",
    "- **Vocabulary**: The set of unique words or tokens that appear in the corpus.\n",
    "- **Words (Tokens)**: The individual units of text, such as words, subwords, or punctuation.\n",
    "\n",
    "These concepts form the basis for most text analysis and NLP tasks, such as text classification, topic modeling, and language generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Tokenization?\n",
    "\n",
    "**Tokenization** is the process of breaking down a piece of text (such as a sentence, paragraph, or document) into smaller units called **tokens**. These tokens can be individual words, subwords, or even characters, depending on the type of tokenization being performed. Tokenization is a fundamental step in **Natural Language Processing (NLP)** because it allows us to transform raw text into a format that can be analyzed and processed by algorithms and models.\n",
    "\n",
    "#### Types of Tokens:\n",
    "1. **Word-level Tokens**: The most common approach, where the text is split into individual words or meaningful units.\n",
    "   - Example: \n",
    "     - Input: \"The cat sat on the mat.\"\n",
    "     - Output: [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]\n",
    "\n",
    "2. **Subword Tokens**: Some tokenization methods break words into smaller meaningful parts, particularly for handling rare words or unknown terms.\n",
    "   - Example (with subword tokenization):\n",
    "     - Input: \"unbelievable\"\n",
    "     - Output: [\"un\", \"believe\", \"able\"]\n",
    "\n",
    "3. **Character-level Tokens**: Here, the text is split into individual characters. This is used in cases where words are too complex or unknown words need to be handled at the character level.\n",
    "   - Example:\n",
    "     - Input: \"cat\"\n",
    "     - Output: [\"c\", \"a\", \"t\"]\n",
    "\n",
    "### Why Tokenization is Important:\n",
    "\n",
    "1. **Preparation for Text Analysis**: Tokenization is often the first step in most NLP tasks (such as text classification, sentiment analysis, machine translation). By breaking text into smaller units, it allows models to analyze and understand text in a structured manner.\n",
    "   \n",
    "2. **Feature Extraction**: In tasks like **document classification**, the individual words or tokens are often used as features for models. For instance, the frequency of each word (token) can be counted to represent a document.\n",
    "\n",
    "3. **Handling Complex Languages**: Tokenization helps handle languages with complex writing systems (such as Chinese or Japanese) where there are no clear word boundaries. It can also manage contractions and hyphenated words in English.\n",
    "\n",
    "### Types of Tokenization Methods:\n",
    "\n",
    "1. **Whitespace Tokenization**:\n",
    "   - This is a simple form of tokenization that splits text based on spaces. While easy to implement, it does not handle punctuation, contractions, or special symbols well.\n",
    "   - Example:\n",
    "     - Input: \"Hello, world!\"\n",
    "     - Output: [\"Hello,\", \"world!\"]\n",
    "\n",
    "2. **Punctuation-based Tokenization**:\n",
    "   - Tokenizes text by splitting at punctuation marks and spaces. This method handles punctuation better than whitespace tokenization.\n",
    "   - Example:\n",
    "     - Input: \"Hello, world!\"\n",
    "     - Output: [\"Hello\", \",\", \"world\", \"!\"]\n",
    "\n",
    "3. **Word Tokenization** (more advanced):\n",
    "   - Advanced word tokenizers use language-specific rules to split words correctly. For example, the NLTK and spaCy libraries in Python provide built-in tokenizers that handle different punctuation marks and contractions accurately.\n",
    "   - Example:\n",
    "     - Input: \"I'm going to the store.\"\n",
    "     - Output: [\"I\", \"'m\", \"going\", \"to\", \"the\", \"store\", \".\"]\n",
    "\n",
    "4. **Subword Tokenization** (Byte Pair Encoding or BPE, WordPiece):\n",
    "   - This approach breaks down uncommon words into smaller, frequent subword units. It is used in modern deep learning models like **BERT** and **GPT**. This method helps in handling out-of-vocabulary words (words not seen during training).\n",
    "   - Example:\n",
    "     - Input: \"unhappiness\"\n",
    "     - Output: [\"un\", \"happiness\"]\n",
    "\n",
    "5. **Sentence Tokenization**:\n",
    "   - Splitting text into individual sentences instead of words. This is useful when the context of full sentences needs to be analyzed.\n",
    "   - Example:\n",
    "     - Input: \"Hello world. How are you?\"\n",
    "     - Output: [\"Hello world.\", \"How are you?\"]\n",
    "\n",
    "### Tokenization in Different NLP Libraries:\n",
    "\n",
    "- **NLTK** (Natural Language Toolkit):\n",
    "  - Provides simple and powerful tokenization methods such as `word_tokenize()` and `sent_tokenize()` for splitting text into words and sentences.\n",
    "  - Example:\n",
    "    ```python\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text = \"The cat sat on the mat.\"\n",
    "    tokens = word_tokenize(text)\n",
    "    print(tokens)  # Output: ['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
    "    ```\n",
    "\n",
    "- **spaCy**:\n",
    "  - Provides a more advanced tokenizer that handles linguistic nuances, such as splitting contractions or treating punctuation correctly.\n",
    "  - Example:\n",
    "    ```python\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(\"The cat sat on the mat.\")\n",
    "    tokens = [token.text for token in doc]\n",
    "    print(tokens)  # Output: ['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
    "    ```\n",
    "\n",
    "- **Transformers (Hugging Face)**:\n",
    "  - Uses subword tokenization methods like BPE or WordPiece, which are particularly useful in pre-trained models like BERT, GPT, and others.\n",
    "  - Example:\n",
    "    ```python\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    text = \"The cat sat on the mat.\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(tokens)  # Output: ['the', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
    "    ```\n",
    "\n",
    "### Challenges in Tokenization:\n",
    "\n",
    "- **Ambiguities**: Tokenizing some languages or complex sentences can lead to ambiguities (e.g., \"I’m\" vs. \"I am\").\n",
    "- **Out-of-Vocabulary Words**: In word-level tokenization, unknown or rare words may not be handled properly unless using subword tokenization techniques.\n",
    "- **Languages Without Clear Boundaries**: In languages like Chinese or Thai, words are not separated by spaces, making tokenization more complex.\n",
    "\n",
    "### Summary:\n",
    "- **Tokenization** is the process of splitting text into smaller units (tokens) such as words, subwords, or characters.\n",
    "- It is a critical step in NLP, enabling models to process and analyze text.\n",
    "- Different methods of tokenization (word-level, subword-level, etc.) are suited for different tasks and languages.\n",
    "- Libraries like NLTK, spaCy, and Hugging Face’s Transformers offer various tools for efficient tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\dlib-19.24.6-py3.12-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\face_recognition-1.3.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\playsound-1.3.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"The space industry is witnessing remarkable developments, with private companies leading the way.\n",
    "Recent missions to Mars and advancements in satellite technology have sparked renewed interest in space exploration.\n",
    "Governments and private enterprises are collaborating to make space travel more accessible in the near future.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The space industry is witnessing remarkable developments, with private companies leading the way.\n",
      "Recent missions to Mars and advancements in satellite technology have sparked renewed interest in space exploration.\n",
      "Governments and private enterprises are collaborating to make space travel more accessible in the near future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **nltk.tokenize** is a module in NLTK that offers several functions and classes for tokenizing text into sentences and words. For sentence tokenization, we use **sent_tokenize**, and for word tokenization, there's **word_tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##  Tokenization\n",
    "## Sentence-->paragraphs\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sent_tokenize Function\n",
    "\n",
    "The sent_tokenize function in NLTK is designed to split a text into sentences. It works by identifying sentence boundaries, which usually involve punctuation marks like periods, exclamation points, or question marks, followed by a space and a capital letter (in most cases).\n",
    "\n",
    "### Steps Involved in Sentence Tokenization:\n",
    "\n",
    "Boundary Detection: The tokenizer looks for common sentence-ending punctuation marks, such as . (period), ! (exclamation mark), ? (question mark).\n",
    "\n",
    "Handling Abbreviations: It is intelligent enough to handle abbreviations like \"Mr.\" or \"Dr.\" and doesn't mistakenly break sentences in such cases.\n",
    "\n",
    "Non-standard Sentences: The function can deal with non-standard sentence structures like those found in social media or informal writing, where sentence boundaries may not be well-defined.\n",
    "\n",
    "\n",
    "### How sent_tokenize Works Under the Hood:\n",
    "\n",
    "1. It uses a pre-trained Punkt sentence tokenizer model. Punkt is an unsupervised machine learning model that learns to identify sentence boundaries based on statistical analysis of the text (e.g., frequency of punctuation).\n",
    "\n",
    "2. This model is trained on various datasets and is highly adaptable to multiple languages, which is why sent_tokenize can work well even on different texts with various linguistic structures.\n",
    "\n",
    "\n",
    "### Practical Use Cases:\n",
    "\n",
    "1. Text Summarization: Before summarizing text, it's often necessary to split it into sentences.\n",
    "\n",
    "2. Question Answering Systems: Sentences need to be parsed individually to answer queries accurately.\n",
    "\n",
    "3. Language Translation: Each sentence might be translated individually in many machine translation systems.\n",
    "\n",
    "4. Text Normalization: Sentence tokenization can be the first step in normalizing text for further NLP tasks like sentiment analysis or entity recognition.\n",
    "\n",
    "### Advantages of sent_tokenize:\n",
    "\n",
    "1. Language Adaptability: It works on multiple languages with minimal modifications.\n",
    "\n",
    "2. Context Awareness: The function is relatively aware of contexts, such as abbreviations, names, and titles, reducing the risk of false splits.\n",
    "\n",
    "3. Efficiency: It provides a quick and effective way to segment text into sentences, which is useful for downstream tasks in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = nltk.tokenize.sent_tokenize(text = corpus, language = \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The space industry is witnessing remarkable developments, with private companies leading the way.',\n",
       " 'Recent missions to Mars and advancements in satellite technology have sparked renewed interest in space exploration.',\n",
       " 'Governments and private enterprises are collaborating to make space travel more accessible in the near future.']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The space industry is witnessing remarkable developments, with private companies leading the way.\n",
      "Recent missions to Mars and advancements in satellite technology have sparked renewed interest in space exploration.\n",
      "Governments and private enterprises are collaborating to make space travel more accessible in the near future.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization \n",
    "## Paragraph-->words\n",
    "## sentence--->words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`word_tokenize`** function from the `nltk.tokenize` module is used for word tokenization, which is the process of breaking down a string of text into individual words or word-like units (tokens). Unlike sentence tokenization (`sent_tokenize`), which focuses on identifying sentence boundaries, **word tokenization** focuses on dividing sentences into smaller components: words, punctuation, or symbols.\n",
    "\n",
    "### 1. **Word Tokenization in General**\n",
    "\n",
    "Word tokenization is essential because many natural language processing tasks, such as sentiment analysis, part-of-speech tagging, and named entity recognition, operate on words rather than whole sentences or documents. Word tokenization takes raw text and splits it into meaningful individual units called **tokens**, which typically consist of:\n",
    "- Words (e.g., \"dog\", \"happy\")\n",
    "- Punctuation (e.g., \".\", \",\", \"!\")\n",
    "- Symbols (e.g., \"@\", \"#\", \"$\")\n",
    "\n",
    "Each of these tokens is treated as an independent unit that can be further analyzed by various NLP algorithms.\n",
    "\n",
    "### 2. **`nltk.tokenize.word_tokenize`**\n",
    "The **`word_tokenize`** function is the primary method for word tokenization in NLTK. It breaks down a sentence into words, treating punctuation and other characters as separate tokens. It's important to note that `word_tokenize` handles punctuation differently from `sent_tokenize` by treating punctuation as individual tokens.\n",
    "\n",
    "#### **Example**:\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hello, world! NLP is awesome. Let's learn it together.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```python\n",
    "['Hello', ',', 'world', '!', 'NLP', 'is', 'awesome', '.', 'Let', \"'s\", 'learn', 'it', 'together', '.']\n",
    "```\n",
    "\n",
    "Here, the `word_tokenize` function breaks the sentence down into words, punctuation marks, and contractions, creating individual tokens for each.\n",
    "\n",
    "### 3. **Steps Involved in Word Tokenization**:\n",
    "\n",
    "- **Splitting**: The text is split based on spaces, punctuation marks, and other word boundaries.\n",
    "\n",
    "- **Handling Punctuation**: Punctuation marks (like periods, commas, and question marks) are treated as individual tokens rather than being attached to words. For example, \"Hello,\" becomes \"Hello\" and \",\".\n",
    "\n",
    "- **Handling Contractions**: Contractions like \"let's\" are split into two tokens: \"let\" and \"'s\".\n",
    "\n",
    "- **Handling Special Symbols**: Symbols like \"$\", \"#\", and \"@\", commonly found in social media or financial texts, are treated as tokens on their own.\n",
    "\n",
    "#### **Under the Hood**:\n",
    "\n",
    "- The `word_tokenize` function relies on **TreebankWordTokenizer**, a tokenizer that mimics the Penn Treebank Tokenization conventions. This tokenizer is more sophisticated than a simple split based on spaces, as it takes into account punctuation, special characters, and certain linguistic rules (like how to treat apostrophes in contractions).\n",
    "\n",
    "- **TreebankWordTokenizer** also follows certain conventions:\n",
    "\n",
    "  - Splits words from punctuation (e.g., \"hello,\" becomes \"hello\" and \",\").\n",
    "\n",
    "  - Treats contractions as separate tokens (e.g., \"don't\" becomes \"do\" and \"n't\").\n",
    "\n",
    "  - Separates some symbols, like dollar signs and percentages, as individual tokens.\n",
    "\n",
    "### 4. **Common Challenges**:\n",
    "\n",
    "Though `word_tokenize` is highly effective, there are certain challenges in word tokenization:\n",
    "\n",
    "- **Ambiguity with Special Characters**: Handling characters like hyphens or apostrophes can be tricky. For instance, should \"rock-and-roll\" be split into three tokens or left as one?\n",
    "\n",
    "- **Different Languages**: Tokenizing non-space-separated languages like Chinese or Japanese requires different techniques, as `word_tokenize` works well primarily for space-separated languages like English.\n",
    "\n",
    "### 5. **Practical Use Cases**:\n",
    "\n",
    "- **Text Preprocessing**: Before feeding text into a machine learning model, tokenizing it into words is often the first step in preprocessing.\n",
    "\n",
    "- **Sentiment Analysis**: In order to analyze sentiment, the text is split into words so that each word’s polarity (positive, negative, or neutral) can be analyzed.\n",
    "\n",
    "- **Part-of-Speech Tagging**: Tokenization is necessary for tagging each word with its part of speech (noun, verb, etc.).\n",
    "\n",
    "- **Named Entity Recognition (NER)**: Tokenization helps in identifying entities like person names, locations, or dates in the text.\n",
    "\n",
    "### 6. **Advantages of `word_tokenize`**:\n",
    "\n",
    "- **Context-Sensitive**: The tokenizer can handle complex text like contractions and punctuation without simply splitting on spaces, making it more sophisticated than a basic `split()` function.\n",
    "\n",
    "- **Efficient**: It's fast and optimized for large datasets, making it suitable for real-world NLP tasks.\n",
    "\n",
    "- **Accurate**: It’s based on the Penn Treebank Tokenizer, which is widely used in linguistics and NLP research.\n",
    "\n",
    "### 7. **Limitations**:\n",
    "- **Simple Heuristics**: It may not always work perfectly with highly irregular text formats or languages without clear word boundaries (like Chinese).\n",
    "\n",
    "- **Context Limitations**: Since it doesn’t use deep language understanding, it may sometimes split words incorrectly in cases of ambiguity.\n",
    "\n",
    "### 8. **Comparing with Other Tokenizers**:\n",
    "In addition to `word_tokenize`, NLTK provides several other tokenizers for different purposes:\n",
    "\n",
    "- **RegexpTokenizer**: Tokenizes text using regular expressions, allowing custom tokenization rules.\n",
    "\n",
    "- **TweetTokenizer**: Specially designed to handle text from social media (like tweets) and manage emoticons, hashtags, and mentions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'space',\n",
       " 'industry',\n",
       " 'is',\n",
       " 'witnessing',\n",
       " 'remarkable',\n",
       " 'developments',\n",
       " ',',\n",
       " 'with',\n",
       " 'private',\n",
       " 'companies',\n",
       " 'leading',\n",
       " 'the',\n",
       " 'way',\n",
       " '.',\n",
       " 'Recent',\n",
       " 'missions',\n",
       " 'to',\n",
       " 'Mars',\n",
       " 'and',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'satellite',\n",
       " 'technology',\n",
       " 'have',\n",
       " 'sparked',\n",
       " 'renewed',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'space',\n",
       " 'exploration',\n",
       " '.',\n",
       " 'Governments',\n",
       " 'and',\n",
       " 'private',\n",
       " 'enterprises',\n",
       " 'are',\n",
       " 'collaborating',\n",
       " 'to',\n",
       " 'make',\n",
       " 'space',\n",
       " 'travel',\n",
       " 'more',\n",
       " 'accessible',\n",
       " 'in',\n",
       " 'the',\n",
       " 'near',\n",
       " 'future',\n",
       " '.']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'space', 'industry', 'is', 'witnessing', 'remarkable', 'developments', ',', 'with', 'private', 'companies', 'leading', 'the', 'way', '.']\n",
      "['Recent', 'missions', 'to', 'Mars', 'and', 'advancements', 'in', 'satellite', 'technology', 'have', 'sparked', 'renewed', 'interest', 'in', 'space', 'exploration', '.']\n",
      "['Governments', 'and', 'private', 'enterprises', 'are', 'collaborating', 'to', 'make', 'space', 'travel', 'more', 'accessible', 'in', 'the', 'near', 'future', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'space',\n",
       " 'industry',\n",
       " 'is',\n",
       " 'witnessing',\n",
       " 'remarkable',\n",
       " 'developments',\n",
       " ',',\n",
       " 'with',\n",
       " 'private',\n",
       " 'companies',\n",
       " 'leading',\n",
       " 'the',\n",
       " 'way',\n",
       " '.',\n",
       " 'Recent',\n",
       " 'missions',\n",
       " 'to',\n",
       " 'Mars',\n",
       " 'and',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'satellite',\n",
       " 'technology',\n",
       " 'have',\n",
       " 'sparked',\n",
       " 'renewed',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'space',\n",
       " 'exploration',\n",
       " '.',\n",
       " 'Governments',\n",
       " 'and',\n",
       " 'private',\n",
       " 'enterprises',\n",
       " 'are',\n",
       " 'collaborating',\n",
       " 'to',\n",
       " 'make',\n",
       " 'space',\n",
       " 'travel',\n",
       " 'more',\n",
       " 'accessible',\n",
       " 'in',\n",
       " 'the',\n",
       " 'near',\n",
       " 'future',\n",
       " '.']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'space',\n",
       " 'industry',\n",
       " 'is',\n",
       " 'witnessing',\n",
       " 'remarkable',\n",
       " 'developments',\n",
       " ',',\n",
       " 'with',\n",
       " 'private',\n",
       " 'companies',\n",
       " 'leading',\n",
       " 'the',\n",
       " 'way.',\n",
       " 'Recent',\n",
       " 'missions',\n",
       " 'to',\n",
       " 'Mars',\n",
       " 'and',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'satellite',\n",
       " 'technology',\n",
       " 'have',\n",
       " 'sparked',\n",
       " 'renewed',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'space',\n",
       " 'exploration.',\n",
       " 'Governments',\n",
       " 'and',\n",
       " 'private',\n",
       " 'enterprises',\n",
       " 'are',\n",
       " 'collaborating',\n",
       " 'to',\n",
       " 'make',\n",
       " 'space',\n",
       " 'travel',\n",
       " 'more',\n",
       " 'accessible',\n",
       " 'in',\n",
       " 'the',\n",
       " 'near',\n",
       " 'future',\n",
       " '.']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`TreebankWordTokenizer`** and **`wordpunct_tokenize`** are two different tokenizers provided by NLTK, and they both serve the purpose of tokenizing text into words. However, they follow different conventions and methods for splitting the text into tokens.\n",
    "\n",
    "### 1. **`TreebankWordTokenizer`**\n",
    "`TreebankWordTokenizer` is a tokenizer that closely follows the tokenization conventions used in the **Penn Treebank**. It is particularly useful for tokenizing standard English text and is more sophisticated than simply splitting on spaces.\n",
    "\n",
    "#### **How `TreebankWordTokenizer` Works**:\n",
    "\n",
    "- **Punctuation Handling**: It splits punctuation from words, treating punctuation marks as separate tokens. For example, `\"can't\"` becomes `['ca', \"n't\"]`, and `\"hello,\"` becomes `['hello', ',']`.\n",
    "\n",
    "- **Contractions**: The tokenizer is designed to handle common English contractions by splitting them into distinct parts. For example, `\"he's\"` becomes `['he', \"'s\"]` and `\"didn't\"` becomes `['did', \"n't\"]`.\n",
    "\n",
    "- **Special Symbols**: It treats dollar signs (`$`), percent signs (`%`), and other special characters separately. For example, `\"$100\"` becomes `['$', '100']`.\n",
    "\n",
    "- **Whitespace Handling**: It correctly handles cases where there is more than one space between words or extra spaces at the beginning or end of a sentence.\n",
    "\n",
    "#### **Example**:\n",
    "```python\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"I can't believe it's already 2024! Let's celebrate.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```python\n",
    "['I', 'can', 'n\\'t', 'believe', 'it', '\\'s', 'already', '2024', '!', 'Let', \"'s\", 'celebrate', '.']\n",
    "```\n",
    "\n",
    "#### **Features of `TreebankWordTokenizer`**:\n",
    "\n",
    "- **Handles Contractions**: It handles apostrophes well, splitting contractions like \"can't\" into `['ca', \"n't\"]`.\n",
    "\n",
    "- **Penn Treebank Standard**: It’s based on the Penn Treebank's tokenization rules, which are widely used in linguistic and NLP research.\n",
    "\n",
    "- **Fine-Grained Tokenization**: It offers more granular control over tokenization by separating punctuation and contractions in a way that mimics how language is typically parsed in treebank datasets.\n",
    "\n",
    "#### **Use Cases**:\n",
    "\n",
    "- **Linguistic Research**: When you're working with datasets like the Penn Treebank corpus.\n",
    "\n",
    "- **Text Parsing**: If you need detailed tokenization with a focus on syntactic parsing (for example, if you're training parsers or POS taggers).\n",
    "\n",
    "- **Text Normalization**: It's useful when you need to split contractions and punctuation marks consistently for further processing, such as in machine translation or text generation.\n",
    "\n",
    "### 2. **`wordpunct_tokenize`**\n",
    "\n",
    "`wordpunct_tokenize` is another tokenizer in NLTK that is simpler and more general-purpose. It splits text based on word boundaries but uses **regular expressions** to separate words from punctuation. The goal is to split the text into alphanumeric words and non-alphanumeric characters (like punctuation marks).\n",
    "\n",
    "#### **How `wordpunct_tokenize` Works**:\n",
    "- **Simple Splitting**: It splits on non-alphanumeric characters, meaning it separates words and punctuation by breaking words on any punctuation mark.\n",
    "\n",
    "- **Punctuation as Separate Tokens**: It treats punctuation (including spaces and symbols) as separate tokens from words.\n",
    "\n",
    "- **No Contraction Handling**: Unlike `TreebankWordTokenizer`, `wordpunct_tokenize` does not handle contractions. Instead, it treats the apostrophe as a separate token. For example, `\"can't\"` becomes `['can', \"'\", 't']`.\n",
    "\n",
    "#### **Example**:\n",
    "```python\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text = \"I can't believe it's already 2024! Let's celebrate.\"\n",
    "tokens = wordpunct_tokenize(text)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```python\n",
    "['I', 'can', \"'\", 't', 'believe', 'it', \"'\", 's', 'already', '2024', '!', 'Let', \"'\", 's', 'celebrate', '.']\n",
    "```\n",
    "\n",
    "#### **Features of `wordpunct_tokenize`**:\n",
    "\n",
    "- **Simplistic**: It’s a very basic and general-purpose tokenizer that works based on non-alphanumeric characters.\n",
    "\n",
    "- **No Contraction Handling**: The tokenizer doesn't have special handling for contractions, unlike `TreebankWordTokenizer`, which splits contractions more intelligently.\n",
    "\n",
    "- **Regular Expression Based**: This tokenizer splits the text using a simple regular expression pattern, which treats any sequence of non-alphabetic characters as a delimiter.\n",
    "  \n",
    "#### **Use Cases**:\n",
    "\n",
    "- **General Tokenization**: When you don’t need sophisticated tokenization (e.g., for quick word counting or simple analysis).\n",
    "\n",
    "- **Social Media Data**: It’s useful in cases where tokenization is less formal, such as in social media, because it easily breaks punctuation and symbols into their own tokens.\n",
    "\n",
    "- **Simple Text Preprocessing**: If you just need a basic tokenizer that splits on punctuation and symbols without advanced linguistic processing.\n",
    "\n",
    "### 3. **Comparison Between `TreebankWordTokenizer` and `wordpunct_tokenize`**:\n",
    "\n",
    "| **Feature**               | **`TreebankWordTokenizer`**                                        | **`wordpunct_tokenize`**                                    |\n",
    "|---------------------------|--------------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Contraction Handling**   | Yes. Handles contractions like `\"can't\"` as `['ca', \"n't\"]`.       | No. Treats contractions like `\"can't\"` as `['can', \"'\", 't']`. |\n",
    "| **Punctuation Handling**   | Splits punctuation from words intelligently.                       | Splits words and punctuation based on regular expressions.    |\n",
    "| **Special Symbols**        | Treats symbols like `$` or `%` as separate tokens.                 | Treats any non-alphanumeric character as a separate token.    |\n",
    "| **Use Case**               | Best for more precise, linguistically-informed tokenization.       | Best for simpler, faster tokenization without deep language understanding. |\n",
    "| **Based on**               | Penn Treebank tokenization rules.                                 | Regular expression-based splitting on non-alphanumeric characters. |\n",
    "| **Sophistication**         | More sophisticated, designed for structured text and formal English.| Simpler and faster for general text, less suited for structured corpora. |\n",
    "\n",
    "\n",
    "\n",
    "- **`TreebankWordTokenizer`** is a sophisticated tokenizer based on Penn Treebank conventions. It’s designed for fine-grained tokenization, handling contractions, punctuation, and special symbols with precision.\n",
    "\n",
    "- **`wordpunct_tokenize`** is a simpler, faster tokenizer that splits words based on non-alphanumeric characters, making it useful for quick, general-purpose tokenization, especially in less structured text like social media or informal writing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
