{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of NLTK and spaCy\n",
    "\n",
    "#### **NLTK (Natural Language Toolkit):**\n",
    "**NLTK** is one of the oldest and most comprehensive libraries for working with human language data (text) in Python. It provides a wide range of tools and resources for text processing, such as tokenization, stemming, lemmatization, part-of-speech (POS) tagging, parsing, and more. Additionally, NLTK includes a large collection of text corpora and lexical resources, which makes it a powerful tool for linguistic research and natural language processing (NLP) tasks.\n",
    "\n",
    "##### Key Features of NLTK:\n",
    "1. **Corpora and Datasets:** Comes with over 50 corpora (e.g., WordNet, Brown Corpus) and lexical resources, making it ideal for research and prototyping.\n",
    "2. **Versatile Text Processing Functions:** Provides extensive tools for text cleaning, stemming, lemmatization, tokenization, tagging, chunking, parsing, and semantic reasoning.\n",
    "3. **Text Classification:** It supports both supervised and unsupervised learning for tasks like document classification, named entity recognition (NER), and more.\n",
    "4. **Parsing & Grammar Tools:** Provides support for both shallow and deep parsing techniques using Context-Free Grammars (CFG), dependency parsing, etc.\n",
    "5. **Rich Ecosystem:** Due to its long history and broad scope, NLTK is widely used in academia for teaching NLP concepts, with numerous tutorials and references available.\n",
    "\n",
    "#### **spaCy:**\n",
    "**spaCy** is a more recent library, designed to be efficient and production-ready for real-world NLP applications. Unlike NLTK, which is more focused on research and learning, spaCy is optimized for speed and ease of use in industry settings. It is designed to work well with large-scale data and is integrated with machine learning frameworks like TensorFlow and PyTorch.\n",
    "\n",
    "##### Key Features of spaCy:\n",
    "1. **Performance and Speed:** spaCy is built with efficiency in mind, using Cython to handle computationally heavy operations, making it faster than most libraries.\n",
    "2. **Pre-trained Models:** Provides state-of-the-art pre-trained models for various languages that support tasks like tokenization, POS tagging, dependency parsing, and named entity recognition (NER).\n",
    "3. **Deep Learning Integration:** Easily integrates with deep learning frameworks (TensorFlow, PyTorch) and includes functionality for training and fine-tuning models.\n",
    "4. **Out-of-the-box NLP Pipelines:** Includes a well-defined and easy-to-extend NLP pipeline that handles all steps of text processing (tokenization, NER, etc.) in a streamlined way.\n",
    "5. **Modern Linguistic Features:** spaCy has advanced support for dependency parsing, word vectors, part-of-speech tagging, and much more. It also provides support for modern techniques like word embeddings (via word2vec, GloVe, etc.).\n",
    "\n",
    "### Differences Between NLTK and spaCy:\n",
    "\n",
    "| **Feature**                | **NLTK**                               | **spaCy**                            |\n",
    "|----------------------------|----------------------------------------|--------------------------------------|\n",
    "| **Release Date**            | Older (first released in 2001)         | Newer (first released in 2015)       |\n",
    "| **Target Audience**         | Researchers, educators, and prototyping| Developers, industry practitioners   |\n",
    "| **Focus**                   | Focus on learning, research, and prototyping | Focus on performance, production, and real-world applications |\n",
    "| **Ease of Use**             | Offers a more academic, modular approach; may require combining various tools manually | Out-of-the-box solutions and streamlined NLP pipelines |\n",
    "| **Performance**             | Slower, not optimized for large-scale data processing | Optimized for high performance with large datasets |\n",
    "| **Pre-trained Models**      | Requires users to train many models; fewer pre-trained models out-of-the-box | Provides robust, pre-trained models (NER, POS, etc.) for various languages |\n",
    "| **Deep Learning**           | Minimal support for deep learning frameworks | Directly integrates with TensorFlow, PyTorch |\n",
    "| **Tokenization**            | Simple rule-based tokenization         | More robust tokenization with better handling of edge cases (via pre-trained models) |\n",
    "| **Language Support**        | Supports many languages via corpora    | Supports multiple languages with specific, pre-trained models |\n",
    "| **Training Custom Models**  | Complex and not as well documented     | Easier to customize and train models, with extensive documentation |\n",
    "| **Parsing and Syntax Trees**| Rich support for different types of parsing (e.g., CFG, dependency parsing) | Dependency parsing is streamlined and fast |\n",
    "| **Stemming & Lemmatization**| Provides traditional stemming algorithms (e.g., Porter Stemmer) | Lemmatization is more accurate due to pre-trained models |\n",
    "| **Community and Ecosystem** | Older, larger academic community with many tutorials and research papers | Growing ecosystem, especially in industry NLP applications |\n",
    "\n",
    "### When to Use NLTK vs. spaCy:\n",
    "- **Use NLTK if:**\n",
    "  - You are learning NLP or working on a research project.\n",
    "  - You need access to a wide variety of text corpora and linguistic data.\n",
    "  - You want to experiment with different NLP techniques, such as different tokenizers or parsers.\n",
    "  \n",
    "- **Use spaCy if:**\n",
    "  - You are working on a large-scale NLP application and need fast, efficient processing.\n",
    "  - You want easy access to pre-trained models for tasks like NER, POS tagging, etc.\n",
    "  - You want to integrate NLP tasks with deep learning models and frameworks.\n",
    "  - You are focused on production-level applications rather than exploratory data analysis.\n",
    "\n",
    "Both libraries are powerful, but spaCy is preferred for production systems due to its speed and ease of use, while NLTK is ideal for educational purposes and linguistic exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP (Natural Language Processing), several fundamental concepts are often used when working with text data. Let’s specify terms like **corpus**, **documents**, **vocabulary**, and **words**:\n",
    "\n",
    "### 1. **Corpus**:\n",
    "A **corpus** (plural: corpora) refers to a large and structured collection of text data, usually in digital form, that is used for linguistic analysis, text mining, or training NLP models. It serves as the base material on which NLP models are trained or analyzed.\n",
    "\n",
    "- **Example of Corpora:**\n",
    "  - **Brown Corpus**: A large corpus of American English text.\n",
    "  - **Wikipedia Dump**: A snapshot of all Wikipedia articles, often used for training NLP models.\n",
    "  - **Movie Reviews Corpus**: A collection of movie reviews, often used for sentiment analysis tasks.\n",
    "\n",
    "In summary, a corpus is a collection of text samples used in research or model training. It could be as small as a set of articles or as large as a database of billions of sentences.\n",
    "\n",
    "### 2. **Documents**:\n",
    "A **document** in NLP refers to an individual piece of text within a corpus. A document could be an article, a paragraph, a tweet, a webpage, a research paper, or any single unit of text.\n",
    "\n",
    "- **Example:**\n",
    "  - In a **news corpus**, each article is considered a document.\n",
    "  - In a **Twitter dataset**, each tweet is treated as a document.\n",
    "\n",
    "Documents are essentially subsets of a corpus.\n",
    "\n",
    "### 3. **Vocabulary**:\n",
    "The **vocabulary** refers to the set of unique words (or tokens) that appear in a corpus. It includes all distinct terms or symbols in the text data. The vocabulary is often generated by tokenizing the corpus (splitting text into words or subwords) and keeping track of unique terms.\n",
    "\n",
    "- **Example:**\n",
    "  If a corpus consists of the following sentences:\n",
    "  - \"The cat sat on the mat.\"\n",
    "  - \"The dog chased the cat.\"\n",
    "  \n",
    "  The vocabulary would include: `{\"The\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"chased\"}` (unique words, ignoring duplicates).\n",
    "  \n",
    "  In many cases, NLP preprocessing involves further reducing the vocabulary by removing common stopwords (like \"the\", \"on\") and applying techniques like lemmatization or stemming.\n",
    "\n",
    "### 4. **Words** (Tokens):\n",
    "In NLP, **words** are the smallest individual units of meaning in the text. These are also called **tokens**. Tokenization is the process of splitting text into individual words or tokens. Each token typically represents a word in the document, though in some cases, a token could be a punctuation mark or even a subword (for example, in languages with complex morphology).\n",
    "\n",
    "- **Example:**\n",
    "  The sentence \"The cat sat on the mat.\" can be tokenized into the following words/tokens:\n",
    "  - [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]\n",
    "\n",
    "A **word** in NLP can be:\n",
    "- A real word from a sentence (\"cat\", \"sat\").\n",
    "- A punctuation mark (\"!\").\n",
    "- A subword if advanced tokenization techniques are used (such as BPE or WordPiece tokenization used in models like BERT).\n",
    "\n",
    "### Example Using All Terms Together:\n",
    "Let’s consider a practical example:\n",
    "1. You have a **corpus** of 1000 news articles.\n",
    "2. Each news article is a **document** in the corpus.\n",
    "3. After processing the text, the **vocabulary** consists of 10,000 unique words.\n",
    "4. The total number of **words** (or tokens) in the corpus is 500,000 (this includes repeated instances of words).\n",
    "\n",
    "### Summary of Definitions:\n",
    "- **Corpus**: A collection of text data.\n",
    "- **Documents**: Individual pieces of text within the corpus.\n",
    "- **Vocabulary**: The set of unique words or tokens that appear in the corpus.\n",
    "- **Words (Tokens)**: The individual units of text, such as words, subwords, or punctuation.\n",
    "\n",
    "These concepts form the basis for most text analysis and NLP tasks, such as text classification, topic modeling, and language generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Tokenization?\n",
    "\n",
    "**Tokenization** is the process of breaking down a piece of text (such as a sentence, paragraph, or document) into smaller units called **tokens**. These tokens can be individual words, subwords, or even characters, depending on the type of tokenization being performed. Tokenization is a fundamental step in **Natural Language Processing (NLP)** because it allows us to transform raw text into a format that can be analyzed and processed by algorithms and models.\n",
    "\n",
    "#### Types of Tokens:\n",
    "1. **Word-level Tokens**: The most common approach, where the text is split into individual words or meaningful units.\n",
    "   - Example: \n",
    "     - Input: \"The cat sat on the mat.\"\n",
    "     - Output: [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]\n",
    "\n",
    "2. **Subword Tokens**: Some tokenization methods break words into smaller meaningful parts, particularly for handling rare words or unknown terms.\n",
    "   - Example (with subword tokenization):\n",
    "     - Input: \"unbelievable\"\n",
    "     - Output: [\"un\", \"believe\", \"able\"]\n",
    "\n",
    "3. **Character-level Tokens**: Here, the text is split into individual characters. This is used in cases where words are too complex or unknown words need to be handled at the character level.\n",
    "   - Example:\n",
    "     - Input: \"cat\"\n",
    "     - Output: [\"c\", \"a\", \"t\"]\n",
    "\n",
    "### Why Tokenization is Important:\n",
    "\n",
    "1. **Preparation for Text Analysis**: Tokenization is often the first step in most NLP tasks (such as text classification, sentiment analysis, machine translation). By breaking text into smaller units, it allows models to analyze and understand text in a structured manner.\n",
    "   \n",
    "2. **Feature Extraction**: In tasks like **document classification**, the individual words or tokens are often used as features for models. For instance, the frequency of each word (token) can be counted to represent a document.\n",
    "\n",
    "3. **Handling Complex Languages**: Tokenization helps handle languages with complex writing systems (such as Chinese or Japanese) where there are no clear word boundaries. It can also manage contractions and hyphenated words in English.\n",
    "\n",
    "### Types of Tokenization Methods:\n",
    "\n",
    "1. **Whitespace Tokenization**:\n",
    "   - This is a simple form of tokenization that splits text based on spaces. While easy to implement, it does not handle punctuation, contractions, or special symbols well.\n",
    "   - Example:\n",
    "     - Input: \"Hello, world!\"\n",
    "     - Output: [\"Hello,\", \"world!\"]\n",
    "\n",
    "2. **Punctuation-based Tokenization**:\n",
    "   - Tokenizes text by splitting at punctuation marks and spaces. This method handles punctuation better than whitespace tokenization.\n",
    "   - Example:\n",
    "     - Input: \"Hello, world!\"\n",
    "     - Output: [\"Hello\", \",\", \"world\", \"!\"]\n",
    "\n",
    "3. **Word Tokenization** (more advanced):\n",
    "   - Advanced word tokenizers use language-specific rules to split words correctly. For example, the NLTK and spaCy libraries in Python provide built-in tokenizers that handle different punctuation marks and contractions accurately.\n",
    "   - Example:\n",
    "     - Input: \"I'm going to the store.\"\n",
    "     - Output: [\"I\", \"'m\", \"going\", \"to\", \"the\", \"store\", \".\"]\n",
    "\n",
    "4. **Subword Tokenization** (Byte Pair Encoding or BPE, WordPiece):\n",
    "   - This approach breaks down uncommon words into smaller, frequent subword units. It is used in modern deep learning models like **BERT** and **GPT**. This method helps in handling out-of-vocabulary words (words not seen during training).\n",
    "   - Example:\n",
    "     - Input: \"unhappiness\"\n",
    "     - Output: [\"un\", \"happiness\"]\n",
    "\n",
    "5. **Sentence Tokenization**:\n",
    "   - Splitting text into individual sentences instead of words. This is useful when the context of full sentences needs to be analyzed.\n",
    "   - Example:\n",
    "     - Input: \"Hello world. How are you?\"\n",
    "     - Output: [\"Hello world.\", \"How are you?\"]\n",
    "\n",
    "### Tokenization in Different NLP Libraries:\n",
    "\n",
    "- **NLTK** (Natural Language Toolkit):\n",
    "  - Provides simple and powerful tokenization methods such as `word_tokenize()` and `sent_tokenize()` for splitting text into words and sentences.\n",
    "  - Example:\n",
    "    ```python\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text = \"The cat sat on the mat.\"\n",
    "    tokens = word_tokenize(text)\n",
    "    print(tokens)  # Output: ['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
    "    ```\n",
    "\n",
    "- **spaCy**:\n",
    "  - Provides a more advanced tokenizer that handles linguistic nuances, such as splitting contractions or treating punctuation correctly.\n",
    "  - Example:\n",
    "    ```python\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(\"The cat sat on the mat.\")\n",
    "    tokens = [token.text for token in doc]\n",
    "    print(tokens)  # Output: ['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
    "    ```\n",
    "\n",
    "- **Transformers (Hugging Face)**:\n",
    "  - Uses subword tokenization methods like BPE or WordPiece, which are particularly useful in pre-trained models like BERT, GPT, and others.\n",
    "  - Example:\n",
    "    ```python\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    text = \"The cat sat on the mat.\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(tokens)  # Output: ['the', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
    "    ```\n",
    "\n",
    "### Challenges in Tokenization:\n",
    "\n",
    "- **Ambiguities**: Tokenizing some languages or complex sentences can lead to ambiguities (e.g., \"I’m\" vs. \"I am\").\n",
    "- **Out-of-Vocabulary Words**: In word-level tokenization, unknown or rare words may not be handled properly unless using subword tokenization techniques.\n",
    "- **Languages Without Clear Boundaries**: In languages like Chinese or Thai, words are not separated by spaces, making tokenization more complex.\n",
    "\n",
    "### Summary:\n",
    "- **Tokenization** is the process of splitting text into smaller units (tokens) such as words, subwords, or characters.\n",
    "- It is a critical step in NLP, enabling models to process and analyze text.\n",
    "- Different methods of tokenization (word-level, subword-level, etc.) are suited for different tasks and languages.\n",
    "- Libraries like NLTK, spaCy, and Hugging Face’s Transformers offer various tools for efficient tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\dlib-19.24.6-py3.12-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\face_recognition-1.3.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\playsound-1.3.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"The space industry is witnessing remarkable developments, with private companies leading the way.\n",
    "Recent missions to Mars and advancements in satellite technology have sparked renewed interest in space exploration.\n",
    "Governments and private enterprises are collaborating to make space travel more accessible in the near future.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The space industry is witnessing remarkable developments, with private companies leading the way.\n",
      "Recent missions to Mars and advancements in satellite technology have sparked renewed interest in space exploration.\n",
      "Governments and private enterprises are collaborating to make space travel more accessible in the near future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##  Tokenization\n",
    "## Sentence-->paragraphs\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = nltk.tokenize.sent_tokenize(text = corpus, language = \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The space industry is witnessing remarkable developments, with private companies leading the way.\n",
      "Recent missions to Mars and advancements in satellite technology have sparked renewed interest in space exploration.\n",
      "Governments and private enterprises are collaborating to make space travel more accessible in the near future.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization \n",
    "## Paragraph-->words\n",
    "## sentence--->words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'space',\n",
       " 'industry',\n",
       " 'is',\n",
       " 'witnessing',\n",
       " 'remarkable',\n",
       " 'developments',\n",
       " ',',\n",
       " 'with',\n",
       " 'private',\n",
       " 'companies',\n",
       " 'leading',\n",
       " 'the',\n",
       " 'way',\n",
       " '.',\n",
       " 'Recent',\n",
       " 'missions',\n",
       " 'to',\n",
       " 'Mars',\n",
       " 'and',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'satellite',\n",
       " 'technology',\n",
       " 'have',\n",
       " 'sparked',\n",
       " 'renewed',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'space',\n",
       " 'exploration',\n",
       " '.',\n",
       " 'Governments',\n",
       " 'and',\n",
       " 'private',\n",
       " 'enterprises',\n",
       " 'are',\n",
       " 'collaborating',\n",
       " 'to',\n",
       " 'make',\n",
       " 'space',\n",
       " 'travel',\n",
       " 'more',\n",
       " 'accessible',\n",
       " 'in',\n",
       " 'the',\n",
       " 'near',\n",
       " 'future',\n",
       " '.']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'space', 'industry', 'is', 'witnessing', 'remarkable', 'developments', ',', 'with', 'private', 'companies', 'leading', 'the', 'way', '.']\n",
      "['Recent', 'missions', 'to', 'Mars', 'and', 'advancements', 'in', 'satellite', 'technology', 'have', 'sparked', 'renewed', 'interest', 'in', 'space', 'exploration', '.']\n",
      "['Governments', 'and', 'private', 'enterprises', 'are', 'collaborating', 'to', 'make', 'space', 'travel', 'more', 'accessible', 'in', 'the', 'near', 'future', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'space',\n",
       " 'industry',\n",
       " 'is',\n",
       " 'witnessing',\n",
       " 'remarkable',\n",
       " 'developments',\n",
       " ',',\n",
       " 'with',\n",
       " 'private',\n",
       " 'companies',\n",
       " 'leading',\n",
       " 'the',\n",
       " 'way',\n",
       " '.',\n",
       " 'Recent',\n",
       " 'missions',\n",
       " 'to',\n",
       " 'Mars',\n",
       " 'and',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'satellite',\n",
       " 'technology',\n",
       " 'have',\n",
       " 'sparked',\n",
       " 'renewed',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'space',\n",
       " 'exploration',\n",
       " '.',\n",
       " 'Governments',\n",
       " 'and',\n",
       " 'private',\n",
       " 'enterprises',\n",
       " 'are',\n",
       " 'collaborating',\n",
       " 'to',\n",
       " 'make',\n",
       " 'space',\n",
       " 'travel',\n",
       " 'more',\n",
       " 'accessible',\n",
       " 'in',\n",
       " 'the',\n",
       " 'near',\n",
       " 'future',\n",
       " '.']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'space',\n",
       " 'industry',\n",
       " 'is',\n",
       " 'witnessing',\n",
       " 'remarkable',\n",
       " 'developments',\n",
       " ',',\n",
       " 'with',\n",
       " 'private',\n",
       " 'companies',\n",
       " 'leading',\n",
       " 'the',\n",
       " 'way.',\n",
       " 'Recent',\n",
       " 'missions',\n",
       " 'to',\n",
       " 'Mars',\n",
       " 'and',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'satellite',\n",
       " 'technology',\n",
       " 'have',\n",
       " 'sparked',\n",
       " 'renewed',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'space',\n",
       " 'exploration.',\n",
       " 'Governments',\n",
       " 'and',\n",
       " 'private',\n",
       " 'enterprises',\n",
       " 'are',\n",
       " 'collaborating',\n",
       " 'to',\n",
       " 'make',\n",
       " 'space',\n",
       " 'travel',\n",
       " 'more',\n",
       " 'accessible',\n",
       " 'in',\n",
       " 'the',\n",
       " 'near',\n",
       " 'future',\n",
       " '.']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
