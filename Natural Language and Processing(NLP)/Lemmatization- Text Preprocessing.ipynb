{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is Lemmatization?**\n",
    "\n",
    "**Lemmatization** is the process of reducing a word to its base or dictionary form, called a **lemma**. Unlike **stemming**, which merely chops off prefixes or suffixes to crudely get to a root form (often resulting in non-real words), lemmatization considers the context and the morphological analysis of the word. This results in more accurate base forms, making lemmatization more sophisticated and meaningful than stemming.\n",
    "\n",
    "For example:\n",
    "- Stemming might reduce the word \"better\" to \"bett,\" but lemmatization would return \"good,\" which is the correct dictionary form (lemma).\n",
    "- Stemming might reduce \"running\" to \"run,\" and lemmatization would also return \"run,\" but in the case of irregular forms like \"ran,\" lemmatization would return the correct lemma \"run.\"\n",
    "\n",
    "### **Lemmatization vs. Stemming**\n",
    "- **Lemmatization** is context-aware and uses a dictionary or lexicon to find the base form of a word.\n",
    "- **Stemming** is simpler but more error-prone, as it cuts off word endings using heuristic rules and often results in invalid words.\n",
    "\n",
    "| **Feature**               | **Lemmatization**              | **Stemming**                  |\n",
    "|---------------------------|-------------------------------|-------------------------------|\n",
    "| **Output**                | Returns dictionary form (lemma) | Returns word stem (may not be a valid word) |\n",
    "| **Complexity**            | More complex (uses linguistic rules) | Simpler (uses rule-based stripping) |\n",
    "| **Accuracy**              | More accurate, context-aware   | Less accurate, sometimes incorrect |\n",
    "| **Language Dependence**   | Language-specific (relies on dictionaries) | Language-independent (rule-based) |\n",
    "\n",
    "### **How Lemmatization Works**:\n",
    "1. **Morphological Analysis**: Lemmatization examines the structure of words and uses part-of-speech tagging to understand the role the word plays in a sentence.\n",
    "2. **Dictionary Lookup**: It refers to a lexicon (a dictionary) to find the base form of the word.\n",
    "3. **Handling Irregular Forms**: Lemmatization can accurately handle irregular word forms, such as \"went\" (lemmatized to \"go\") or \"better\" (lemmatized to \"good\").\n",
    "\n",
    "### **Classes in NLTK for Lemmatization**\n",
    "\n",
    "In the **NLTK (Natural Language Toolkit)** library, there are different tools and classes for performing lemmatization. The two primary ones are:\n",
    "\n",
    "1. **WordNet Lemmatizer**\n",
    "2. **TextBlob Lemmatizer (an external library built on top of NLTK)**\n",
    "\n",
    "### **1. WordNet Lemmatizer**\n",
    "The **WordNet Lemmatizer** is the most common tool used in NLTK for lemmatization. It relies on the **WordNet** corpus, which is a large lexical database of English, to determine the lemma of a word.\n",
    "\n",
    "#### Key Features:\n",
    "- Handles different parts of speech (POS).\n",
    "- Returns the lemma of a word by consulting the WordNet corpus.\n",
    "- Takes into account the POS tag for better accuracy (verbs, nouns, adjectives, etc.).\n",
    "\n",
    "#### Example Code:\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to get the WordNet POS tag from standard POS tags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character WordNetLemmatizer accepts\"\"\"\n",
    "    from nltk import pos_tag\n",
    "    from nltk.corpus import wordnet\n",
    "    \n",
    "    # Get the POS tag for the word\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    \n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Lemmatize words with appropriate POS tag\n",
    "words = [\"running\", \"better\", \"happier\", \"ate\", \"flying\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "\n",
    "# Display original and lemmatized words\n",
    "for original, lemmatized in zip(words, lemmatized_words):\n",
    "    print(f\"Original: {original} -> Lemmatized: {lemmatized}\")\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "1. **POS Tagging**: Since lemmatization depends on the part of speech (POS), it is useful to identify whether a word is a noun, verb, adjective, or adverb. The `get_wordnet_pos()` function helps map regular POS tags (from NLTK's `pos_tag()` function) to the format that the **WordNet Lemmatizer** accepts.\n",
    "2. **Lemmatization**: The `lemmatize()` method from `WordNetLemmatizer` uses WordNet’s dictionary to return the base form of the word based on its POS.\n",
    "\n",
    "### Example Output:\n",
    "```\n",
    "Original: running -> Lemmatized: run\n",
    "Original: better -> Lemmatized: good\n",
    "Original: happier -> Lemmatized: happy\n",
    "Original: ate -> Lemmatized: eat\n",
    "Original: flying -> Lemmatized: fly\n",
    "```\n",
    "\n",
    "#### Explanation of Results:\n",
    "- \"running\" → \"run\": Verbs are reduced to their base form.\n",
    "- \"better\" → \"good\": The irregular comparative is correctly lemmatized to its adjective root.\n",
    "- \"happier\" → \"happy\": Comparatives and superlatives are reduced to the base form.\n",
    "- \"ate\" → \"eat\": Past tense is lemmatized to the base form.\n",
    "\n",
    "### **2. TextBlob Lemmatizer**\n",
    "While **TextBlob** isn't part of NLTK directly, it builds on top of NLTK and simplifies tasks like lemmatization and POS tagging. If you're working in environments where you want quick and easy lemmatization, **TextBlob** can be a good choice.\n",
    "\n",
    "#### Installation:\n",
    "You can install TextBlob via pip:\n",
    "```bash\n",
    "pip install textblob\n",
    "```\n",
    "\n",
    "#### Example Code:\n",
    "```python\n",
    "from textblob import Word\n",
    "\n",
    "# Lemmatize words using TextBlob\n",
    "words = [\"running\", \"better\", \"happier\", \"ate\", \"flying\"]\n",
    "lemmatized_words = [Word(word).lemmatize() for word in words]\n",
    "\n",
    "# Display original and lemmatized words\n",
    "for original, lemmatized in zip(words, lemmatized_words):\n",
    "    print(f\"Original: {original} -> Lemmatized: {lemmatized}\")\n",
    "```\n",
    "\n",
    "### Example Output:\n",
    "```\n",
    "Original: running -> Lemmatized: run\n",
    "Original: better -> Lemmatized: better\n",
    "Original: happier -> Lemmatized: happier\n",
    "Original: ate -> Lemmatized: ate\n",
    "Original: flying -> Lemmatized: flying\n",
    "```\n",
    "\n",
    "#### Explanation:\n",
    "TextBlob’s lemmatizer works similarly to WordNet but is less powerful when it comes to irregular forms like \"better\" or \"ate.\" You might still want to use NLTK’s **WordNetLemmatizer** for more accurate results in complex cases.\n",
    "\n",
    "---\n",
    "\n",
    "### **Other Important Classes and Libraries for Lemmatization**\n",
    "\n",
    "1. **spaCy**:\n",
    "   spaCy offers a high-performance NLP pipeline and is widely used for industrial applications. Its lemmatizer is powerful and efficient.\n",
    "   \n",
    "   Installation:\n",
    "   ```bash\n",
    "   pip install spacy\n",
    "   python -m spacy download en_core_web_sm\n",
    "   ```\n",
    "   \n",
    "   Example Code:\n",
    "   ```python\n",
    "   import spacy\n",
    "   nlp = spacy.load(\"en_core_web_sm\")\n",
    "   \n",
    "   # Process text using spaCy\n",
    "   doc = nlp(\"running better happier ate flying\")\n",
    "   \n",
    "   # Lemmatize each word in the text\n",
    "   for token in doc:\n",
    "       print(f\"Original: {token.text}, Lemma: {token.lemma_}\")\n",
    "   ```\n",
    "\n",
    "   Output:\n",
    "   ```\n",
    "   Original: running, Lemma: run\n",
    "   Original: better, Lemma: well\n",
    "   Original: happier, Lemma: happy\n",
    "   Original: ate, Lemma: eat\n",
    "   Original: flying, Lemma: fly\n",
    "   ```\n",
    "\n",
    "2. **Stanford CoreNLP**:\n",
    "   This is another robust library for lemmatization and other NLP tasks. It provides high-quality results but is more resource-intensive.\n",
    "\n",
    "3. **Pattern Library**:\n",
    "   The Pattern library also includes a lemmatizer. It is more lightweight than spaCy and is useful for simple tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "Lemmatization is a critical step in many **NLP tasks** such as **text classification**, **information retrieval**, and **machine translation**, where reducing words to their base form can help normalize the input and improve model performance. Compared to stemming, lemmatization is more accurate and context-aware, making it preferable for tasks requiring semantic understanding.\n",
    "\n",
    "- **WordNet Lemmatizer** in **NLTK** is the go-to tool for lemmatization, especially when combined with proper **POS tagging**.\n",
    "- **TextBlob** provides a simpler interface but may not handle all edge cases like irregular verbs and adjectives.\n",
    "- More advanced libraries like **spaCy** provide high-performance lemmatization suited for production-level NLP tasks.\n",
    "\n",
    "By choosing the appropriate tool and understanding its strengths, you can effectively incorporate lemmatization into your **NLP pipeline** for better text processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\dlib-19.24.6-py3.12-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\face_recognition-1.3.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\playsound-1.3.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Harsh\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "## Q&A,chatbots,text summarization\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "POS- Noun-n\n",
    "verb-v\n",
    "adjective-a\n",
    "adverb-r\n",
    "'''\n",
    "lemmatizer.lemmatize(\"going\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eat\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->history\n",
      "finally---->finally\n",
      "finalized---->finalize\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goes\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'sportingly')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"fairly\",pos='v'),lemmatizer.lemmatize(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
