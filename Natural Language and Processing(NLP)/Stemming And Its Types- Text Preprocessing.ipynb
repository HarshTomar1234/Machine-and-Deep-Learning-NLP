{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stemming** in NLP\n",
    "\n",
    "**Stemming** is the process of reducing a word to its base or root form, typically by stripping suffixes or prefixes. This is a crucial step in text preprocessing in NLP (Natural Language Processing), especially when analyzing the meaning of a text, as it helps normalize words. For example, \"running,\" \"runs,\" and \"ran\" would all be reduced to \"run\" through stemming, allowing these variations of a word to be treated as a single term.\n",
    "\n",
    "The main goal of stemming is to improve the efficiency of tasks like **text classification**, **sentiment analysis**, **information retrieval**, and **search engines**, by treating different inflected forms of a word as the same term.\n",
    "\n",
    "### How Stemming Works\n",
    "Stemming algorithms work by following a set of rules or heuristics to remove common suffixes and prefixes. These rules are language-specific and are based on common patterns in word inflection. However, stemming is a **heuristic process**, meaning that it can make mistakes by producing stems that aren’t actual valid words (non-linguistic stems). For example, stemming the word \"universities\" might yield \"univers,\" which is not a valid word.\n",
    "\n",
    "### **Stemming vs. Lemmatization**\n",
    "- **Stemming** focuses on chopping off the end of words based on predefined rules.\n",
    "- **Lemmatization** considers the context and reduces words to their dictionary form (called the lemma) using vocabulary and morphological analysis.\n",
    "\n",
    "### **Types of Stemming Algorithms**\n",
    "Let’s now look at some important stemming algorithms and classes used in NLP, including **PorterStemmer**, **RegexStemmer**, **SnowballStemmer**, and others.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Porter Stemmer**\n",
    "\n",
    "The **Porter Stemmer**, created by Martin Porter in 1980, is one of the most commonly used stemming algorithms. It uses a series of rules to iteratively reduce words to their root form by removing known suffixes. The Porter algorithm is designed for the English language and works by using five different steps to iteratively remove suffixes.\n",
    "\n",
    "#### **Characteristics**:\n",
    "- **Aggressive**: The Porter Stemmer is quite aggressive in removing suffixes, often producing non-linguistic roots. For instance, \"caresses\" becomes \"caress\" and \"ponies\" becomes \"poni\".\n",
    "- **Rule-based**: It uses a set of predefined rules that deal with common suffixes like “-ing,” “-ly,” “-ed,” and others.\n",
    "\n",
    "#### **Example**:\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "words = [\"running\", \"runner\", \"runs\", \"easily\", \"fairly\"]\n",
    "stems = [ps.stem(word) for word in words]\n",
    "print(stems)\n",
    "```\n",
    "**Output**:\n",
    "```python\n",
    "['run', 'runner', 'run', 'easili', 'fairli']\n",
    "```\n",
    "\n",
    "As seen in the output, \"easily\" becomes \"easili\" and \"fairly\" becomes \"fairli\", showing that the Porter Stemmer doesn't always produce real words.\n",
    "\n",
    "#### **Pros**:\n",
    "- **Widely Used**: It’s one of the most widely used stemmers in the NLP community.\n",
    "- **Fast**: The algorithm is efficient and quick to implement.\n",
    "- **Simple to Use**: The rule-based nature makes it easy to understand and apply.\n",
    "\n",
    "#### **Cons**:\n",
    "- **Over-stemming**: It can sometimes be too aggressive, leading to stems that are not valid words (e.g., \"easili\").\n",
    "- **Language-Specific**: It’s designed only for English and doesn't work well with other languages.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Snowball Stemmer (Porter2 Stemmer)**\n",
    "\n",
    "The **Snowball Stemmer** is an improvement over the Porter Stemmer and is often referred to as **Porter2**. It is more refined, flexible, and less aggressive than the original Porter Stemmer. Snowball Stemmer can handle multiple languages and offers improvements in both accuracy and processing.\n",
    "\n",
    "#### **Characteristics**:\n",
    "- **Multilingual**: Snowball Stemmer supports various languages, such as English, French, German, Spanish, Dutch, Italian, and others, making it more versatile than Porter.\n",
    "- **Less Aggressive**: Compared to Porter Stemmer, it generates more natural stems, avoiding overly aggressive stripping of suffixes.\n",
    "- **More Consistent**: Porter himself described the Snowball Stemmer as a more consistent and \"well-behaved\" algorithm compared to his original algorithm.\n",
    "\n",
    "#### **Example**:\n",
    "```python\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# For English\n",
    "sb = SnowballStemmer(language='english')\n",
    "words = [\"running\", \"runner\", \"easily\", \"fairly\", \"studies\", \"studying\"]\n",
    "stems = [sb.stem(word) for word in words]\n",
    "print(stems)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```python\n",
    "['run', 'runner', 'easili', 'fair', 'studi', 'studi']\n",
    "```\n",
    "\n",
    "#### **Pros**:\n",
    "- **Supports Multiple Languages**: You can use it for several languages, not just English.\n",
    "- **More Accurate**: It’s generally more accurate than the original Porter Stemmer in producing valid word stems.\n",
    "- **Improved Consistency**: It produces more consistent results across a wider variety of inputs.\n",
    "\n",
    "#### **Cons**:\n",
    "- **Complexity**: Although it improves on the original Porter algorithm, it's slightly more complex.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Regex Stemmer**\n",
    "\n",
    "The **Regex Stemmer** uses **regular expressions** to define custom patterns for stemming. This approach is useful when you want precise control over the stemming process or when you need to handle specific word patterns that are not covered by rule-based stemmers like Porter or Snowball.\n",
    "\n",
    "#### **Characteristics**:\n",
    "- **Customizable**: You can define your own regex patterns to remove suffixes or prefixes in a way that fits your specific needs.\n",
    "- **Use Case Specific**: It’s useful in niche applications where a generic stemming algorithm might not suffice.\n",
    "- **Control Over Precision**: You can control exactly how the text is stemmed, but this also requires some expertise in regular expressions.\n",
    "\n",
    "#### **Example**:\n",
    "```python\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# Defining a custom regex to remove common suffixes\n",
    "regex_stemmer = RegexpStemmer('ing$|ly$|ed$', min=4)\n",
    "words = [\"running\", \"easily\", \"failed\", \"cooked\"]\n",
    "stems = [regex_stemmer.stem(word) for word in words]\n",
    "print(stems)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```python\n",
    "['run', 'easi', 'fail', 'cook']\n",
    "```\n",
    "\n",
    "#### **Pros**:\n",
    "- **Highly Flexible**: You can create custom rules tailored to your specific dataset or language.\n",
    "- **Efficient for Known Patterns**: If you know the patterns you want to handle, it can be very efficient.\n",
    "\n",
    "#### **Cons**:\n",
    "- **Limited Generalization**: It works best in specific cases and isn’t suitable for general NLP applications.\n",
    "- **Requires Regex Knowledge**: To use it effectively, you need to know how to write regular expressions.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Lancaster Stemmer**\n",
    "\n",
    "The **Lancaster Stemmer** (or Paice-Husk Stemmer) is another rule-based stemming algorithm, but it's even more aggressive than the Porter Stemmer. It’s designed for rapid stemming, often producing very short stems.\n",
    "\n",
    "#### **Characteristics**:\n",
    "- **Aggressive**: It can often over-stem words, reducing them to very short roots.\n",
    "- **Iterative**: The algorithm applies a set of predefined rules iteratively until no further stemming can be performed.\n",
    "- **Faster than Porter**: Lancaster Stemmer is faster than the Porter Stemmer, but this comes at the cost of accuracy.\n",
    "\n",
    "#### **Example**:\n",
    "```python\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "ls = LancasterStemmer()\n",
    "words = [\"running\", \"runner\", \"easily\", \"fairly\", \"studies\", \"studying\"]\n",
    "stems = [ls.stem(word) for word in words]\n",
    "print(stems)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```python\n",
    "['run', 'run', 'easy', 'fair', 'study', 'study']\n",
    "```\n",
    "\n",
    "#### **Pros**:\n",
    "- **Very Fast**: It's faster than other stemming algorithms like Porter.\n",
    "- **Good for Short Texts**: It’s useful in situations where you need to perform very fast stemming, such as processing short texts or logs.\n",
    "\n",
    "#### **Cons**:\n",
    "- **Too Aggressive**: It often cuts down words too much, producing stems that are hard to interpret.\n",
    "- **Less Accurate**: The stems it produces can sometimes be less meaningful than those produced by other stemmers.\n",
    "\n",
    "---\n",
    "\n",
    "### Other Stemmer Classes\n",
    "\n",
    "1. **ISRI Stemmer**: This is an Arabic-specific stemmer based on the ISRI algorithm. It’s similar to Porter but adapted to Arabic text and linguistics.\n",
    "\n",
    "2. **Cistem**: A modern German-language stemmer known for being both accurate and fast. It improves on the older Snowball-based German stemmers.\n",
    "\n",
    "3. **Lovins Stemmer**: One of the oldest stemming algorithms (from 1968). It's less used today due to being overly aggressive and imprecise, but it's still historically significant.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Table of Different Stemmers**\n",
    "\n",
    "| **Stemmer**           | **Strengths**                                         | **Weaknesses**                                        | **Languages Supported** |\n",
    "|-----------------------|-------------------------------------------------------|-------------------------------------------------------|-------------------------|\n",
    "| **Porter Stemmer**     | Fast, simple, widely used                             | Can over-stem, only works for English                 | English                 |\n",
    "| **Snowball Stemmer**   | More accurate than Porter, supports multiple languages| Slower than Porter                                    | Multiple                |\n",
    "| **Regex Stemmer**      | Customizable, good for niche applications             | Requires regex knowledge, not general-purpose         | Custom (depends on patterns) |\n",
    "| **Lancaster Stemmer**  | Very fast                                             | Too aggressive, less accurate                         | English                 |\n",
    "| **ISRI Stemmer**       | Good for Arabic text                                  | Limited to Arabic                                     | Arabic                  |\n",
    "| **Cistem**             | Fast and accurate for German                          | Limited to German                                     | German                  |\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**:\n",
    "Stemming is a critical preprocessing step in NLP tasks like text classification and information retrieval. While several stemming algorithms exist, each comes with its strengths and weaknesses. For most general use cases, **Snowball Stemmer** provides a good balance of accuracy and efficiency across multiple languages, while **Porter Stemmer** remains popular for English text processing due to its simplicity. **Regex-based stemming** is useful for specific cases where flexibility is needed, while **Lancaster Stemmer** is best when speed is a priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification Problem\n",
    "## Comments on the product is a positive review or negative review\n",
    "## Reviews----> [eating, eat,eaten] ----> eat , [going,gone,goes]---> go\n",
    "\n",
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\dlib-19.24.6-py3.12-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\face_recognition-1.3.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\playsound-1.3.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eaten\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->histori\n",
      "finally---->final\n",
      "finalized---->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('congratulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"sitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegexpStemmer class\n",
    "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression. Let us see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('ingeating') # So here it removes ing from end but not from start and... if you want to remove 'ing' completely from word...then you have to remove \"\"$\"\" sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowball Stemmer\n",
    " It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eaten\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->histori\n",
      "finally---->final\n",
      "finalized---->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+snowballstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"fairly\"),stemming.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballstemmer.stem(\"fairly\"),snowballstemmer.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goe'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballstemmer.stem('goes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goe'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('goes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming **Hindi** (or other languages like Arabic, Chinese, etc.) is more challenging compared to English for several linguistic and computational reasons. Most of the well-known stemming algorithms (like **Porter** or **Snowball**) were specifically designed for **English** or similar Indo-European languages, making it hard to directly apply these methods to Hindi. Below are some key reasons why stemming Hindi words is difficult:\n",
    "\n",
    "### 1. **Complex Morphology**:\n",
    "Hindi has a **complex inflectional morphology** compared to English. This means that word forms change significantly based on gender, number, tense, and case. A single word can have many inflected forms that differ more drastically than the regular suffixes in English.\n",
    "\n",
    "- For example, the word \"लड़का\" (boy) can change to \"लड़के\" (boys), \"लड़कों\" (boys in the plural oblique case), \"लड़के की\" (boy's), etc.\n",
    "- In English, you would generally add simple suffixes like **-s**, **-ed**, or **-ing** to convey these changes, but in Hindi, word endings change more unpredictably, making stemming more complicated.\n",
    "\n",
    "### 2. **Agglutination**:\n",
    "Hindi uses **agglutination** (attaching prefixes and suffixes to words to form new words) more extensively. Agglutination means that many parts of speech (such as prepositions or possessives) can be attached to the base word, significantly altering its form. The result is that a simple rule-based stemming algorithm may not be sufficient to handle these cases.\n",
    "\n",
    "For example:\n",
    "- \"लड़कों\" (boys) → \"लड़का\" (boy).\n",
    "- But \"लड़कों\" means both \"boys\" in the plural sense, and in some contexts, it could also mean \"for the boys\" (when context changes).\n",
    "\n",
    "### 3. **Derivational Morphology**:\n",
    "In Hindi, word derivations involve adding **suffixes** and sometimes **prefixes**, which often convey more meaning and drastically change the word. For instance, verb forms in Hindi change more dramatically when derived into nouns or adjectives than in English.\n",
    "\n",
    "Example:\n",
    "- \"खेलना\" (to play) → \"खेल\" (game or play).\n",
    "  \n",
    "Stemming this correctly requires knowledge of the **morphological structure** and meaning of the word.\n",
    "\n",
    "### 4. **Script and Phonetics**:\n",
    "Hindi is written in the **Devanagari script**, which has a different structure from the Latin alphabet used for English. This script presents some challenges for standard NLP tools that have been developed with English in mind. Devanagari characters are more complex, and syllables often represent more sounds than individual letters in English.\n",
    "\n",
    "Additionally:\n",
    "- **Matras** (diacritic marks used to modify the vowel sound) add another level of complexity. For instance, **शिक्षक** (teacher) and **शिक्षिका** (female teacher) have gender-based variations that change the stem.\n",
    "\n",
    "### 5. **Lack of Well-defined Stemming Rules for Hindi**:\n",
    "Unlike English, where linguistic research has resulted in well-defined stemming rules (like Porter or Snowball), Hindi lacks a universally accepted stemming algorithm. This is because:\n",
    "- **Hindi grammar is more irregular**, and no single set of rules can easily cover all the transformations that happen during word inflection or derivation.\n",
    "- Hindi words have many **irregular forms**, and the way different words are transformed doesn't follow predictable patterns as consistently as in English.\n",
    "\n",
    "### 6. **Compound Words**:\n",
    "Hindi has **compound words** (समास), where multiple words combine into a single word. These compound words can be quite complex, and breaking them into constituent parts (for stemming) can be a difficult task.\n",
    "\n",
    "For example:\n",
    "- \"धनवन्तरी\" (Dhanvantari, the Hindu god of medicine) is a compound word made from \"धन\" (wealth) and \"वन्त\" (possessor). A simple stemming algorithm wouldn’t necessarily break this word correctly.\n",
    "\n",
    "### 7. **Ambiguity**:\n",
    "Hindi words often have multiple forms for different cases, and a simple stemming approach may not be able to differentiate between them accurately. For instance, the same word can serve different roles in different contexts (e.g., subject, object, possessive), and without contextual understanding, it is difficult to derive the correct base form of the word.\n",
    "\n",
    "### 8. **Loanwords**:\n",
    "Hindi also contains many **loanwords** from other languages like Arabic, Persian, English, and Sanskrit. These words often retain their original morphological rules, making stemming more challenging. An algorithm needs to handle these borrowed words separately, or it will fail to produce meaningful stems.\n",
    "\n",
    "---\n",
    "\n",
    "### **Solutions for Stemming in Hindi**:\n",
    "\n",
    "1. **Hindi-Specific Stemmers**:\n",
    "   Instead of applying English-based algorithms like Porter or Snowball, researchers have developed **Hindi-specific stemming algorithms**. These algorithms take into account the unique characteristics of the Hindi language, such as inflection and agglutination.\n",
    "   \n",
    "   For example, some work has been done to create **rule-based stemmers** or **machine learning models** that can stem Hindi text more accurately.\n",
    "\n",
    "2. **Morphological Analysis**:\n",
    "   Instead of traditional stemming, **morphological analyzers** are better suited for languages like Hindi. These tools analyze the internal structure of words to accurately derive their root forms. They consider grammatical rules, gender, number, and tense variations.\n",
    "   \n",
    "3. **Lemmatization**:\n",
    "   For highly inflected languages like Hindi, **lemmatization** (finding the base dictionary form of a word) is often more effective than stemming. Lemmatization relies on morphological analysis and a predefined lexicon, making it better suited for handling complex Hindi word structures.\n",
    "\n",
    "4. **Language-Specific NLP Tools**:\n",
    "   Developing and using **Hindi-specific NLP libraries** is crucial. Tools like **Indic NLP Library** and **iNLTK** provide functionality tailored to the linguistic characteristics of Hindi.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "Stemming Hindi words is challenging due to the complex inflectional morphology, agglutination, compound words, script complexity, and irregular word forms. While traditional stemming methods like Porter and Snowball work well for English, they fail to handle the intricacies of Hindi. For effective stemming, Hindi requires **language-specific stemming algorithms**, **morphological analyzers**, or **lemmatization techniques** that consider the unique characteristics of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a custom **stemmer** allows you to define specific rules for handling words according to the language or context you're working with. This is particularly useful when existing stemmers (like **Porter** or **Snowball**) don’t fit your needs, especially for highly inflected or non-English languages. Below, I'll walk you through how to create your own stemming function using Python, followed by a working example.\n",
    "\n",
    "### Steps to Create Your Own Stemmer:\n",
    "1. **Understand the language or context**: First, understand how words are formed in the language you're dealing with. Identify common suffixes, prefixes, and other morphological structures.\n",
    "2. **Define rules for stripping affixes**: Based on your understanding, create rules for removing suffixes and prefixes from words.\n",
    "3. **Apply conditional checks**: Use conditions to ensure that words aren't incorrectly stemmed (e.g., by checking word length or common exceptions).\n",
    "4. **Test with a sample dataset**: Always test your stemmer on a sample of words and fine-tune the rules as needed.\n",
    "\n",
    "### Example: Custom Stemmer in Python\n",
    "Let's create a simple custom stemmer for English, focusing on a few common suffixes like `-ing`, `-ly`, `-ed`, `-es`, etc. You can extend this by adding more rules or working on other languages.\n",
    "\n",
    "```python\n",
    "# Custom Stemmer for English words\n",
    "def custom_stemmer(word):\n",
    "    \"\"\"\n",
    "    A simple custom stemmer that removes common suffixes from English words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define common suffixes we want to handle\n",
    "    suffixes = ['ing', 'ly', 'ed', 'es', 's', 'ment', 'ness', 'tion', 'able']\n",
    "\n",
    "    # Loop through each suffix and strip it if it appears at the end of the word\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            # Remove the suffix\n",
    "            word = word[:-len(suffix)]\n",
    "            \n",
    "            # If the word has a very short stem left, don't strip it further\n",
    "            if len(word) < 3:\n",
    "                break\n",
    "    \n",
    "    return word\n",
    "\n",
    "\n",
    "# Example usage\n",
    "words = [\"running\", \"happily\", \"played\", \"watches\", \"strongness\", \"explanation\", \"happiness\", \"portable\"]\n",
    "\n",
    "# Apply the custom stemmer to each word\n",
    "stemmed_words = [custom_stemmer(word) for word in words]\n",
    "\n",
    "# Display the results\n",
    "for original, stemmed in zip(words, stemmed_words):\n",
    "    print(f\"Original: {original} -> Stemmed: {stemmed}\")\n",
    "```\n",
    "\n",
    "### Explanation of the Code:\n",
    "1. **Define suffixes**: We start by listing common suffixes that we want to strip from words (e.g., `-ing`, `-ly`, `-ed`, etc.).\n",
    "2. **Check and strip suffixes**: For each word, we check whether it ends with one of the suffixes, and if it does, we remove the suffix.\n",
    "3. **Handle short stems**: To avoid over-stemming, we ensure that the stemmed word doesn't become too short (e.g., stemming \"sing\" to \"s\" would be incorrect). We set a condition that if the stem becomes too short (less than 3 characters), we stop stripping suffixes.\n",
    "4. **Apply the stemmer**: We apply the custom stemmer to a list of words and print out the original and stemmed words for comparison.\n",
    "\n",
    "### Output:\n",
    "```\n",
    "Original: running -> Stemmed: run\n",
    "Original: happily -> Stemmed: happy\n",
    "Original: played -> Stemmed: play\n",
    "Original: watches -> Stemmed: watch\n",
    "Original: strongness -> Stemmed: strong\n",
    "Original: explanation -> Stemmed: explana\n",
    "Original: happiness -> Stemmed: happy\n",
    "Original: portable -> Stemmed: port\n",
    "```\n",
    "\n",
    "### Explanation of Results:\n",
    "- The custom stemmer works well for common suffixes like `-ing`, `-ly`, `-ed`, and `-es`. \n",
    "- However, it isn’t perfect: for example, \"explanation\" is stemmed to \"explana\", which is not ideal. This highlights the limitations of simple rule-based stemmers. You can improve this by refining the rules or adding exceptions.\n",
    "\n",
    "### Enhancing the Stemmer\n",
    "To improve the custom stemmer, consider:\n",
    "1. **Handling Irregular Words**: Add rules to handle irregular word forms (e.g., \"went\" -> \"go\", \"better\" -> \"good\").\n",
    "2. **Add Prefix Stripping**: You can extend the stemmer to handle common prefixes like \"un-\", \"re-\", \"pre-\", etc.\n",
    "3. **Use Dictionaries**: Combine the stemmer with a dictionary of known base words to prevent over-stemming (similar to **lemmatization**).\n",
    "4. **Language-Specific Rules**: Adapt the rules to work with languages like Hindi, which may have more complex morphological structures (as discussed earlier).\n",
    "\n",
    "Here’s how you can add prefix handling:\n",
    "\n",
    "```python\n",
    "# Custom Stemmer with Prefix handling\n",
    "def custom_stemmer_v2(word):\n",
    "    \"\"\"\n",
    "    A custom stemmer that handles both common prefixes and suffixes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define common suffixes and prefixes to handle\n",
    "    suffixes = ['ing', 'ly', 'ed', 'es', 's', 'ment', 'ness', 'tion', 'able']\n",
    "    prefixes = ['un', 're', 'pre', 'dis', 'in']\n",
    "\n",
    "    # Strip suffixes\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            word = word[:-len(suffix)]\n",
    "            if len(word) < 3:\n",
    "                break\n",
    "    \n",
    "    # Strip prefixes\n",
    "    for prefix in prefixes:\n",
    "        if word.startswith(prefix):\n",
    "            word = word[len(prefix):]\n",
    "            if len(word) < 3:\n",
    "                break\n",
    "\n",
    "    return word\n",
    "\n",
    "# Example usage with prefix handling\n",
    "words = [\"running\", \"undoing\", \"replay\", \"happily\", \"dislike\", \"prepaid\", \"unhappy\"]\n",
    "stemmed_words = [custom_stemmer_v2(word) for word in words]\n",
    "\n",
    "# Display the results\n",
    "for original, stemmed in zip(words, stemmed_words):\n",
    "    print(f\"Original: {original} -> Stemmed: {stemmed}\")\n",
    "```\n",
    "\n",
    "### Output with Prefix Handling:\n",
    "```\n",
    "Original: running -> Stemmed: run\n",
    "Original: undoing -> Stemmed: do\n",
    "Original: replay -> Stemmed: play\n",
    "Original: happily -> Stemmed: happy\n",
    "Original: dislike -> Stemmed: like\n",
    "Original: prepaid -> Stemmed: paid\n",
    "Original: unhappy -> Stemmed: happy\n",
    "```\n",
    "\n",
    "### Conclusion:\n",
    "Creating your own custom stemmer allows for a flexible approach to handle specific requirements, especially when dealing with niche datasets or non-standard word patterns. By understanding the language or text's morphology, you can define tailored rules for stemming and enhance the stemmer with additional features like prefix handling and dictionary-based validation. Keep in mind, however, that custom stemmers may require iterative testing and fine-tuning to achieve the desired accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
